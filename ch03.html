<section data-type="chapter" id="tactile_sensations" xmlns="http://www.w3.org/1999/xhtml">
<h1>Tactile Sensations</h1>

<p>This <a contenteditable="false" data-primary="touch" data-seealso="haptic technology" data-type="indexterm" id="t3">&nbsp;</a><a contenteditable="false" data-primary="tactile sensations" data-see="touch" data-type="indexterm">&nbsp;</a>next wave of Augmented Reality (AR) explores creating new sensorial experiences that include the other senses beyond just vision. Touch in AR has the possibility to not only synchronize what we see with how something feels, it has the potential of creating new ways to communicate using tactility. From the “taptic engine” in the <a contenteditable="false" data-primary="Apple" data-secondary="Apple Watch" data-type="indexterm">&nbsp;</a>Apple Watch providing subtle tap feedback for things like notifications, to new types of haptic handheld controllers being designed for <a contenteditable="false" data-primary="Virtual Reality (VR)" data-type="indexterm">&nbsp;</a>Virtual Reality (VR) applications, like those by <a href="http://tacticalhaptics.com/">Tactical Haptics</a> to <a contenteditable="false" data-primary="Tactical Haptics" data-type="indexterm">&nbsp;</a>bring new levels of realism to VR, we’re beginning to see a trend toward digital tactility.</p>

<p>In the physical world, you can use your hands to touch something, pick something up, or make something. In AR, the virtual appears to exist in your physical space, but if you reach your hand out to touch a virtual object, depending on whether you’re using a smartphone or eyewear, you will either feel glass or thin air.</p>

<p>In <a data-type="xref" href="#a_new_wave_of_reality">#a_new_wave_of_reality</a>, <em>registration</em> was referenced as a way to seamlessly align virtual objects in three-dimensional (3-D) space in the real world. <a contenteditable="false" data-primary="registration" data-type="indexterm" id="reg3">&nbsp;</a>Registration in AR is currently focused on visual alignment, but what about the other senses? If one of the goals of AR is to provide a seamless environment, this perception becomes shattered when the user attempts to touch the virtual, and is left feeling nothing. This next wave of AR makes touching the virtual possible, further blurring our ability to distinguish between the real and the virtual.</p>

<p>Touch helps us to navigate and comprehend the real world. Our sense of touch helps us to understand something on a deeper level by feeling things like texture and weight. This leads to knowledge: what something is made of, and how it compares to other tactile things. Touch allows us to verify that an object exists physically.</p>

<p>I believed that last statement to be true until my senses were taken for a loop the day I tried <a contenteditable="false" data-primary="haptic technology" data-type="indexterm" id="ht3">&nbsp;</a>haptics (technology that provides tactile feedback) for the first time in 2011 at the Magic Vision Lab at the University of South Australia. I recall the moment during which I struggled to identify what was physically real and what was virtual. I was flabbergasted. I was able to look at and touch a virtual fish, feeling the individual scales of the fish as though it were real. It was a completely new and bewildering sensation to be able to touch the virtual and receive tactile feedback from something that did not physically exist in the real world. How was this possible?</p>

<p>Wearing a head-mounted display (HMD) and using a haptic device called PHANTOM desktop, <a contenteditable="false" data-primary="PHANTOM desktop" data-type="indexterm">&nbsp;</a>which had a pen-like attachment I held in my hand, I could touch and feel virtual objects that appeared in my physical surroundings. This haptic device simulated touch at a single point of contact using three small motors that gave force feedback by exerting pressure on the stylus. In addition to texture, it was also possible to feel weight using this tool.</p>

<p>It was a highly believable illusion: what I saw actually corresponded to what I felt. Sight and touch are closely coupled in reality, but there is a disconnect between the two for the most part in AR. This forever changed my experience of AR and expanded my thoughts on how this new medium would evolve in the future.</p>

<p>In 2011, I designed and built <em><a href="https://vimeo.com/25608606">Who’s Afraid of Bugs?</a></em>, the world’s first AR pop-up book using image recognition for the iPad. <a contenteditable="false" data-primary="Augmented Reality (AR)" data-secondary="pop-up book" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="books, pop-up" data-type="indexterm">&nbsp;</a>The book combines the art of paper engineering (the cutting, gluing, and folding of paper) with the magic of AR to create a physical pop-up storybook exploring a fear of bugs. When the book was viewed through an iPad, or <a contenteditable="false" data-primary="smartphones" data-type="indexterm">&nbsp;</a>smartphone, various virtual critters appeared, including a hairy tarantula that walked across your hand. I created the book before experiencing the haptics demo at the Magic Vision Lab in Australia, but I can easily see how haptics could be integrated into the next iteration of the book. It could be used to create a greater fear-inducing experience of not only seeing a spider crawl over you hand, but actually feeling the weight and texture of the spider on your skin.</p>

<p>As with the aforementioned virtual fish example, it is possible to re-create how things feel in the physical world and apply that to virtual objects as a means to further enhance “registration.” In advancing AR as a new experience medium, it is important that we also consider and explore new ways in which we can use augmented touch beyond merely replicating physical <a contenteditable="false" data-primary="registration" data-startref="reg3" data-type="indexterm">&nbsp;</a>reality.</p>

<p>For example, can we create experiences with contrasting tactile properties where something appears to be soft, but feels sharp? How can we move beyond the screen to experience touch in new ways? And how else can we use tactile stimulation as a means to communicate in nonverbal ways? In this chapter, we explore research and innovation in haptic technology that will help answer these questions.</p>

<section data-type="sect1" id="haptics_and_touchscreens">
<h1>Haptics and Touchscreens</h1>

<p>The haptics demo <a contenteditable="false" data-primary="touch" data-secondary="haptics and touchscreens" data-type="indexterm" id="t3hat">&nbsp;</a><a contenteditable="false" data-primary="touchscreens" data-type="indexterm" id="ts3">&nbsp;</a><a contenteditable="false" data-primary="haptic technology" data-secondary="and touchscreens" data-secondary-sortas="touchscreens" data-type="indexterm" id="ht3at">&nbsp;</a>I experienced in 2011 at the Magic Vision Lab used expensive and clunky equipment not accessible to the average person. Most AR experiences currently use <a contenteditable="false" data-primary="smartphones" data-secondary="and haptics" data-secondary-sortas="haptics" data-type="indexterm" id="sp3hat">&nbsp;</a>smartphones or tablets, with a major shift beginning to happen with consumer eyewear becoming available in the near future. Eyewear changes the possibilities for a tactile experience in AR from touching the glass screen on your smartphone or tablet, to now reaching out in front of you and interacting with touch in new ways.</p>

<p>In <em>A Brief Rant on the Future of Interaction Design</em> (2011),<span data-type="footnote">Bret Victor, <a href="http://worrydream.com/ABriefRantOnTheFutureOfInteractionDesign/">“A Brief Rant on the Future of Interaction Design.”</a></span> User Interface (UI) designer and Human–Computer Interaction (HCI) researcher <a contenteditable="false" data-primary="Victor, Bret" data-type="indexterm">&nbsp;</a>Bret Victor observes how most future interaction concepts completely ignore that our hands feel and manipulate things. He comments on how almost every object in the world offers some form of tactile feedback—whether it’s weight, texture, pliability, or edges—and responds in your hand as you use it. Yet, he says devices like the iPad “sacrifice all the tactile richness of working with our hands.” Victor calls for a future of interaction that is “a dynamic medium that we can see, feel, and manipulate.”</p>

<p>How far have we come since 2011 and Victor’s article? In 2015 Apple <a contenteditable="false" data-primary="Apple" data-secondary="taptic engine" data-type="indexterm">&nbsp;</a>introduced the taptic engine on <a contenteditable="false" data-primary="iPhone" data-secondary="and tactile feedback" data-secondary-sortas="tactile" data-type="indexterm">&nbsp;</a>iPhones <a contenteditable="false" data-primary="Apple" data-secondary="iPhone" data-type="indexterm">&nbsp;</a>and iPads, enabling tactile feedback for the user. We’re beginning to see tactile feedback devices and controllers emerge as an area of active development in VR for <a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="for gaming" data-secondary-sortas="gaming" data-type="indexterm">&nbsp;</a>gaming, as well, and we’ll likely see those tools adapted for AR experiences in gaming and entertainment in the near future.</p>

<p>Senseg’s E-Sense technology, <a contenteditable="false" data-primary="Senseg" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="E-Sense technology" data-type="indexterm">&nbsp;</a>which premiered at the Consumer Electronics Show (CES) in 2012, presents one way to integrate haptics into AR with a tablet or smartphone.</p>

<p>Dave Rice, <a contenteditable="false" data-primary="Rice, Dave" data-type="indexterm">&nbsp;</a>vice president of Senseg, a Finland-based startup, describes the technology as adding tactile effects to touchscreen displays including smartphones, tablet computers, touch pads, and gaming devices. He discusses the possibilities for gaming applications, referring to a treasure hunt game in which a treasure chest is hidden and can be found only by feeling around on the screen. Rice says,<span data-type="footnote"><a href="https://youtu.be/FiCqlYKRlAA">&#x201c;New Technology: Haptic Feedback for Touchscreens.&#x201d;</a></span> “There were no visual cues there and that’s pretty exciting because now we can move to the world of feel to complement what you’re seeing, or to work independently from it and really create a new world to explore.”</p>

<p>E-Sense works by using electrostatic fields to fool our sense of touch and simulate different levels of friction, allowing it to generate the sensation of texture on a flat screen. The technology uses the <a contenteditable="false" data-primary="Coulomb force" data-type="indexterm">&nbsp;</a><em>Coulomb force</em>: the attraction or repulsion of objects or particles because of their electric charge. An example of this is when you rub a balloon against your hair and it sticks. When rubbing the balloon on your head, electrons are transferred from your hair: your hair is positively charged, the balloon is negatively charged, and the opposite charges attract. Senseg creates an attractive force between your finger and the screen. By modulating this force, a variety of sensations can be generated, to give different textures to different images.</p>

<p>Imagine using this technology on a smartphone or tablet to experience a virtual petting zoo in your home, and being able to feel the softness of a sheep. Touch can now correspond with what you are looking at in AR; the virtual no longer needs to feel “glassy.”</p>

<p>Fujitsu Labs <a contenteditable="false" data-primary="Fujitsu Labs" data-type="indexterm">&nbsp;</a>in Japan is another company working on haptics for touchscreens. The company demonstrated a prototype of a haptic sensory tablet at Mobile World Congress 2014 in Barcelona, Spain, showing how the technology can simulate 3-D formations like bumps, ridges, and protrusions on touchscreen surfaces. The demonstration allowed you to feel the sensations of turning a lock, touching sand, and playing the strings on a musical instrument.</p>

<p>Instead of electrostatic haptic feedback, Fujitsu Labs uses ultrasonic vibrations to convey tactile sensations, which can be pulsed with varying force. The vibrations work by pushing your finger off of the tablet’s surface, and depending on strength, it can simulate various textures. A rapid variation of pulses between low and high friction can create a rough or bumpy feel, whereas a surface can feel slippery with a high-pressure layer of air to reduce friction. Fujitsu Labs plans to commercialize the technology, with online shopping as one of the use cases in which you can feel the fabrics of the items you are <a contenteditable="false" data-primary="smartphones" data-secondary="and haptics" data-secondary-sortas="haptics" data-startref="sp3hat" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="touch" data-secondary="haptics and touchscreens" data-startref="t3hat" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="touchscreens" data-startref="ts3" data-type="indexterm">&nbsp;</a>&nbsp;<a contenteditable="false" data-primary="haptic technology" data-startref="ht3" data-type="indexterm">&nbsp;</a>buying.</p>
</section>

<section data-type="sect1" id="deformable_screens">
<h1>Deformable Screens</h1>

<p>Both Senseg and Fujitsu <a contenteditable="false" data-primary="touch" data-secondary="deformable screens" data-type="indexterm" id="t3ds">&nbsp;</a><a contenteditable="false" data-primary="deformable screens" data-type="indexterm" id="ds3">&nbsp;</a>simulate tactile effects atop flat touchscreens, but what if touchscreens could dynamically deform and physically take the shape of the images or objects represented? Imagine using your hands to manipulate and pull virtual objects and data directly out of a 2-D display and into the 3-D world.</p>

<p>GHOST (Generic and Highly Organic Shape-changing inTerfaces) is a <a contenteditable="false" data-primary="GHOST (Generic and Highly Organic Shape-Changing InTerface)" data-type="indexterm">&nbsp;</a>research project that launched in 2013 across four universities in the United Kingdom, Netherlands, and Denmark, exploring shape-changing displays that you can touch and feel. The researchers built a flatscreen display out of Lycra, which, unlike glass, can be deformed at will and allows you to reach in and touch an object or data.</p>

<p>Kasper Hornbæk, a <a contenteditable="false" data-primary="Hornbaek, Kasper" data-type="indexterm">&nbsp;</a>project researcher from the University of Copenhagen, says, “Nearly all screens are square so most of the interaction is via the changes to the screen. We want to explore screens that can have an arbitrary shape and can change shape by themselves.” This echoes Victor referring to a computer screen as a dynamic visual medium that can visually represent almost anything, and that a dynamic tactile medium then, by extension, also should be able to represent almost anything, now in a tangible way.</p>

<p>This deformable display technology could allow a surgeon, for instance, to physically reach into a virtual brain, engaging in a fully tactile experience before performing a real life operation. <a contenteditable="false" data-primary="artist" data-type="indexterm">&nbsp;</a>Artists and designers using physical materials such as clay could continually remold objects using their hands and store them in a computer as they work. Hornbæk suggests that such a display could also allow you to hold the hand of your significant other, even if they are on another continent.</p>

<p>Esben Warming Pedersen, a member of <a contenteditable="false" data-primary="Pedersen, Esben Warming" data-type="indexterm">&nbsp;</a>the research team at the University of Copenhagen, explains how the deformable display differs from the way normal glass touchscreens work. “All that the iPad actually sees is the tip of your finger touching the glass display. So, when an iPad tries to find out where and how we touch it, you can think of the iPad actually as a coordinate system.” The deformable display is more complex: when you press your finger into the display, a camera captures 3-D depth data of the position and the pressure of your finger on the screen. Pedersen is working to develop computer vision algorithms that make it possible to take this 3-D data and represent it in a way so the computer can better understand and apply it in interactions.</p>

<p>One challenge Pedersen identifies is that we don’t yet know how to interact with these new screens. He comments on how we have an existing common vocabulary to interact with 2-D displays, such as pinching our fingers to zoom out of a picture, and sliding to switch to another picture, but if we then look at 3-D gestures, or deformable gestures, it’s less apparent how to use these screens. Pedersen is working on user studies in search of an intuitive vocabulary of new gestures.</p>

<p>Pedersen and Hornbæk published<span data-type="footnote">Giovanni Maria Troiano, Esben Warming Pedersen, Kasper Hornbæk, <a href="http://www.kasperhornbaek.dk/papers/AVI2014_Gestures.pdf">“User-Defined Gestures for Elastic, Deformable Displays,”</a> <em>Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces</em>, (2014): 1–8.</span> a guessability study in 2014, which involved asking participants to perform gestures they found appropriate for completing multiple tasks such as selection, navigation, and 3-D modeling on the deformable screen. Some of the gestures the participants in the study suggested included reaching behind the display, pushing with a flat hand, and grabbing <a contenteditable="false" data-primary="deformable screens" data-startref="ds3" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="touch" data-secondary="deformable screens" data-startref="t3ds" data-type="indexterm">&nbsp;</a>and twisting.</p>
</section>

<section data-type="sect1" id="adding_a_sense_of_touch_beyond_the_scree">
<h1>Adding a Sense of Touch Beyond the Screen</h1>

<p>Disney Research Labs <a contenteditable="false" data-primary="touch" data-secondary="beyond touchscreens" data-type="indexterm" id="t3bt">&nbsp;</a>takes a different <a contenteditable="false" data-primary="Disney Research Labs" data-type="indexterm" id="drl3">&nbsp;</a>approach to haptics, moving away from screens to explore other interaction experiences. <a href="https://www.disneyresearch.com/project/revel-programming-the-sense-of-touch/">REVEL</a>, developed in 2012 by Ivan Poupyrev <a contenteditable="false" data-primary="Poupryev, Ivan" data-type="indexterm">&nbsp;</a>and <a contenteditable="false" data-primary="Bau, Olivier" data-type="indexterm">&nbsp;</a>Olivier Bau, can provide artificial tactile sensations not only on touchscreens, but everyday objects such as furniture, walls, wooden and plastic objects, and even human skin.</p>

<p>REVEL utilizes a new tactile effect that Disney Research Labs calls <a contenteditable="false" data-primary="haptic technology" data-secondary="Reverse Electrovibration" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Reverse Electrovibration" data-type="indexterm">&nbsp;</a>Reverse Electrovibration. The device injects a weak electrical signal into the user’s body, creating an oscillating electrical field around the user’s fingers. When the user slides their fingers on an object’s surface, tactile textures are perceived that augment the object. A variety of tactile sensations can be created by varying the signal properties.</p>

<p>A flat plastic object can be made to feel rough and bumpy, even though it is smooth and slippery. REVEL can be applied to AR to add texture to virtual content that is projected on tables and walls, or seen through AR eyewear. REVEL also can be used without eyewear or projections, to augment existing objects, such as glass display cases in museums, allowing the user to feel artifacts that might be fragile and irreplaceable, and otherwise inaccessible to touch. Further, REVEL can be customized to <a contenteditable="false" data-primary="customizable experiences" data-type="indexterm">&nbsp;</a>each user, and can even be used to reveal private personalized content, such as password hints that you can feel.</p>

<p>In 2013, Disney Research Labs also <a contenteditable="false" data-primary="AIREAL (Interactive Tactile Experience in Free Air)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="haptic technology" data-secondary="AIREAL (Interactive Tactile Experience in Free Air)" data-type="indexterm">&nbsp;</a>developed <a href="https://www.disneyresearch.com/project/aireal/">AIREAL: Interactive Tactile Experiences in Free Air</a>. AIREAL delivers tactile sensations in the air without having to wear or touch a physical device. This is done by stimulating the user’s skin with compressed air pressure fields using air vortices (rings of air) enabling users to both see and feel projected images.</p>

<p>Depth image sensors are integrated into the AIREAL design, with the user’s hands, head, and body tracked in 3-D for interaction. For example, a projected 3-D butterfly can be displayed hovering on the user’s hand. The motion of the user’s hand and arm is tracked by AIREAL and the direction of the vortices are adjusted to match the movement of the butterfly’s wings. Early user feedback indicated,<span data-type="footnote">Rajinder Sodhi, Matthew Glisson, and Ivan Poupyrev, <a href="http://www.disneyresearch.com/wp-content/uploads/Aireal_FNL1.pdf">“AIREAL: Interactive Tactile Experiences in Free Air,”</a> <em>ACM Transactions on Graphics (TOG) - SIGGRAPH 2013 Conference Proceedings</em>, (2013).</span> “the interaction provided compelling physical sensations of the virtual butterfly,” which one user described as, “it feels natural, it feels like a butterfly <a contenteditable="false" data-primary="Disney Research Labs" data-startref="drl3" data-type="indexterm">&nbsp;</a>should feel.”</p>

<p><a href="https://www.ultrahaptics.com/">UltraHaptics</a> also <a contenteditable="false" data-primary="UltraHaptics" data-type="indexterm" id="uh3">&nbsp;</a><a contenteditable="false" data-primary="haptic technology" data-secondary="UltraHaptics" data-type="indexterm" id="ht3uh">&nbsp;</a>allows you to feel virtual objects in midair. Whereas AIREAL works by blowing small rings of air at a user to simulate touch, UltraHaptics uses high-frequency ultrasound waves. Ultrahaptics was developed by computer scientists at the University of Bristol in 2013 as part of the <a contenteditable="false" data-primary="GHOST (Generic and Highly Organic Shape-Changing InTerface)" data-type="indexterm">&nbsp;</a>GHOST project, and was spun off as a startup to develop the technology for commercial use.</p>

<p>With UltraHaptics, an infrared sensor tracks the precise position of a user’s fingers in 3-D space and enables ultrasound to be accurately directed at a user’s hands, producing the sensation of touch. The company envisions a range of applications for the technology including interacting with moving objects in <a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="for gaming" data-secondary-sortas="gaming" data-type="indexterm">&nbsp;</a>VR games (which also can be extended to AR), to midair controls for automotive dashboards. Applied in the home, the technology can be used to avoid touching kitchen appliance controls with dirty hands while cooking.</p>

<p>The company is also working on safety solutions and has partnered with Jaguar Land Rover in the investigation of a midair touch system for its Predictive Infotainment Screen to reduce driver distraction by minimizing the amount of time the driver’s eyes and hands are on the screen. Using the UltraHaptics solution, the driver’s hand can be located and tracked as it moves across the interactive field, with the system creating a physical sensation to indicate connection. You can feel switches and buttons, with the ability to control them in midair and receive feedback to confirm their action has been successfully completed, all without looking at the <a contenteditable="false" data-primary="touch" data-secondary="beyond touchscreens" data-startref="t3bt" data-type="indexterm">&nbsp;</a>display.</p>
</section>

<section data-type="sect1" id="haptics_as_a_method_of_communication">
<h1>Haptics as a Method of Communication</h1>

<p>The possibilities for <a contenteditable="false" data-primary="touch" data-secondary="communicating with" data-type="indexterm" id="t3cw">&nbsp;</a><a contenteditable="false" data-primary="haptic technology" data-secondary="as communication method" data-secondary-sortas="communication" data-type="indexterm" id="ht3acm">&nbsp;</a>interaction systems like UltraHaptics aren’t limited to simulating controls or the feel of virtual objects. Marianna Obrist, <a contenteditable="false" data-primary="Obrist, Marianna" data-type="indexterm">&nbsp;</a>a scientist and lecturer at the Department of Informatics at the University of Sussex, is using UltraHaptics to explore the communication of <a contenteditable="false" data-primary="emotive communication" data-type="indexterm" id="ec3">&nbsp;</a>emotions. Obrist writes:</p>

<blockquote>
<p>Touch is a powerful vehicle for communication between humans. There is a growing trend to design interactive systems that elicit and support emotions beyond facial and vocal channels. In particular, communicating and mediating emotions through touch is an area of research that opens up new design opportunities for emotion-related communication.</p>
</blockquote>

<p>Obrist has pinpointed how next-generation technologies like UltraHaptics can stimulate different areas of the hand to convey feelings of happiness, sadness, excitement, or fear. Short, sharp bursts of air to the area around the thumb, index finger, and middle part of the palm generate excitement, and sad feelings are created by slow and moderate stimulation of the outer palm and the area around the little finger.</p>

<p>Obrist uses a hypothetical example of a couple that just had a fight in the morning before going to work. While in a meeting, the woman receives a gentle sensation transmitted through her bracelet into the middle of her palm. The sensation comforts her and indicates that her partner is no longer angry.</p>

<p>Obrist believes the technology has a variety of application opportunities. It can open up new ways of communication not only for people who are blind and deaf, but to anyone. It could be applied either for one-to-one interactions, such as a discrete tactile system between a couple or friends, or it could be used for one-to-many interactions, to create tactile sensations for larger groups such as in a cinema for more immersive viewing <a contenteditable="false" data-primary="UltraHaptics" data-startref="uh3" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="haptic technology" data-secondary="UltraHaptics" data-startref="ht3uh" data-type="indexterm">&nbsp;</a>experiences.</p>

<p><a href="http://www.smartstones.co/">Smartstones</a>, a company <a contenteditable="false" data-primary="Smartstones" data-type="indexterm" id="ss3">&nbsp;</a>based in <a contenteditable="false" data-primary="haptic technology" data-secondary="Smartstones" data-type="indexterm" id="ht3ss">&nbsp;</a>Santa Barbara, California, is working on a tactile language system for friends and loved ones to connect nonverbally. The Smartstones platform allows you to configure a library of messages and send and receive them using simple touch gestures. The hardware device, called Touch, is shaped like a river stone and can be worn as a pendant or wristband, or held in the palm of the hand. Messages are received on the Touch as a unique vibration and LED pattern combination referred to as a “Hapticon.”</p>

<p>Each stone includes Bluetooth connectivity, gyroscope, LED lights, speaker, a capacitive touch interface (which recognizes and responds to light touches by your finger), and a gesture recognition library. Though a smartphone isn’t required for use, the Touch interfaces with an app that allows you to program it to respond to specific gestures. You can use it to create your own personalized communication system, and even send secret messages. The stone could be programmed to send a loved one a message communicating “thinking of you” when tapped twice, or that you’re “feeling anxious” if rubbed with a thumb. Other gestures it recognizes include swipes, taps, and <a contenteditable="false" data-primary="emotive communication" data-startref="ec3" data-type="indexterm">&nbsp;</a>shakes.</p>

<p>Although Smartstones is designed to be used by everyone, the product was originally intended for seniors who had a stroke or a neurological disease such as ALS. Smartstones has also gained interest from parents who have children with autism. One of the initial goals for the device was to give a voice to people who aren’t able to communicate verbally, and to be able to do so quickly in a simple way, without the time required to learn something like braille or sign language. “We are essentially creating a platform for understanding and communication continuity for many people,” says<span data-type="footnote">Andreas Forsland, <a href="https://www.linkedin.com/pulse/augmenting-life-unlocking-minds-andreas-forsland">“Augmenting life. Unlocking minds.,”</a> <em>Linkedin</em>, June 5, 2015.</span> Andreas Forsland, Smartstones <a contenteditable="false" data-primary="Forsland, Andreas" data-type="indexterm">&nbsp;</a>founder and CEO. “Our focus today is on Human-to-Human communication—increasing a human’s capacity to connect and thrive. Specifically, for people with speech impairments like autism, ALS, aphasia, as well as the general population.” Using gesture, Smartstones can send data from stone to stone, from stone to text, or even stone to <a contenteditable="false" data-primary="haptic technology" data-secondary="Smartstones" data-startref="ht3ss" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Smartstones" data-startref="ss3" data-type="indexterm">&nbsp;</a>voice.</p>

<p>Both Obrist’s and Forsland’s work point to technology supporting a more visceral mode of digital communication and new forms of user interactions with our hands, representative of language and emotion. As our everyday lives become increasingly digital, these projects help us to regain and perhaps even redefine a sense of human touch in a virtual age by assigning new meanings to tactility, and designing a new way of understanding the world and one <a contenteditable="false" data-primary="touch" data-secondary="communicating with" data-startref="t3cw" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="haptic technology" data-secondary="as communication method" data-secondary-sortas="communication" data-startref="ht3acm" data-type="indexterm">&nbsp;</a>another.</p>
</section>

<section data-type="sect1" id="a_brave_new_world_and_the_feelies">
<h1>A Brave New World and the Feelies</h1>

<p>Rather <a contenteditable="false" data-primary="cinematic experiences" data-seealso="film" data-type="indexterm" id="ce3">&nbsp;</a><a contenteditable="false" data-primary="touch" data-secondary="sensory cinematic experiences" data-type="indexterm" id="t3sce">&nbsp;</a>than touching objects, real or virtual, with only your fingertip, what if you could feel what characters are experiencing in a story with your entire body? In &#x201c;A Brief Rant on the Future of Interaction Design,&#x201d; <a contenteditable="false" data-primary="Victor, Bret" data-type="indexterm">&nbsp;</a>Victor also writes,<span data-type="footnote">Bret Victor, <a href="http://worrydream.com/ABriefRantOnTheFutureOfInteractionDesign/">“A Brief Rant on the Future of Interaction Design.”</a></span> “With an entire body at your command, do you seriously think the Future of Interaction should be a single finger?<em>”</em></p>

<p>Felix Heibeck, Alexis Hope, Julie Legault, and Sophia Brueckner, researchers at the MIT Media Lab, created a<a contenteditable="false" data-primary="haptic technology" data-secondary="Sensory Fiction prototype" data-type="indexterm">&nbsp;</a> <a contenteditable="false" data-primary="Sensory Fiction prototype" data-type="indexterm">&nbsp;</a>book that you <a contenteditable="false" data-primary="Brueckner, Sophia" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Legault, Julie" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Hope, Alexis" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Heibeck, Felix" data-type="indexterm">&nbsp;</a>can wear and experience with your entire body. <a href="http://bit.ly/2u8hgLt">The sensory fiction</a> prototype consists of a haptic vest that is connected to an ebook allowing the reader to feel the protagonist’s physiological emotions. The wearable device can change sound, lighting, temperature, chest tightness, and even the heart rate of the reader to mirror what the main character in the book is experiencing.</p>

<blockquote>
<p>“Going to the Feelies this evening, Henry?” enquired the Assistant Predestinator. “I hear the new one at the Alhambra is first-rate. There’s a love scene on a bearskin rug; they say it’s marvelous. Every hair of the bear reproduced. The most amazing tactual effects.</p>

<p data-type="attribution">Aldous Huxley</p>
</blockquote>

<p>The Sensory Fiction prototype echoes Aldous Huxley’s science fiction novel, <em>A Brave New World</em> (1932), which depicted an entertainment experience enabling spectators to “feel” fictional narratives. <a contenteditable="false" data-primary="film" data-secondary="&quot;Feelies&quot;" data-type="indexterm">&nbsp;</a>The “Feelies” were films that combined a sense of touch with visuals and sounds. Cinema-goers would grab hold of metal orbs attached to the armrest of their chairs to feel tactile sensations mirroring the action of the characters on screen.</p>

<p>Going back to <a contenteditable="false" data-primary="Obrist, Marianna" data-type="indexterm">&nbsp;</a>Obrist’s idea that haptics could be used in a cinema to create a more immersive viewing experience, I could see her work potentially being applied in the following way. Rather than tactile sensations mirroring the actions of the characters like in the Feelies, such as feeling the hair of the bearskin rug (as in the quotation), the emotional state of the character could be translated through short, sharp bursts of air to the middle of the spectator’s hand (as in Obrist’s research) indicating the character’s excitement. This also could be done without the accompanying visuals in film and applied to audio or radio plays. An “emotions touchtrack” (a term I’ve imagined here for the sake of this example, but is something that does not yet exist) becomes possible, building new ways to connect with and further empathize with a character through not only seeing and hearing, but feeling how they are feeling by emotions translated through tactile feedback.</p>

<p>The work of Obrist, Forsland, <a contenteditable="false" data-primary="Forsland, Andreas" data-type="indexterm">&nbsp;</a>and the MIT researchers present ways of forming a deeper connection with a person, or character, by translating a sentiment, mood, message, or even story, in nonverbal ways. This leads to a new way of knowing and understanding through feeling with the human body. But how else can this modality of feeling information be used to interpret and understand other types <a contenteditable="false" data-primary="cinematic experiences" data-seealso="film" data-startref="ce3" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="touch" data-secondary="sensory cinematic experiences" data-type="indexterm">&nbsp;</a>of data?</p>
</section>

<section data-type="sect1" id="sensory_substitution">
<h1>Sensory Substitution</h1>

<p>Neuroscientist <a contenteditable="false" data-primary="touch" data-secondary="sensory substitution" data-type="indexterm" id="t3ss">&nbsp;</a><a contenteditable="false" data-primary="sensory substitution" data-type="indexterm" id="ssub3">&nbsp;</a><a contenteditable="false" data-primary="haptic technology" data-secondary="sensory substitution" data-type="indexterm" id="ht3sens">&nbsp;</a>David Eagleman is <a contenteditable="false" data-primary="Eagleman, David" data-type="indexterm" id="ef3">&nbsp;</a>working on expanding a person’s ability to experience data in a new intuitive way—by embodying information that is felt through a special <a contenteditable="false" data-primary="haptic technology" data-secondary="VEST (Versatile Extra-Sensory Transducer)" data-type="indexterm" id="ht3v">&nbsp;</a>haptics vest.</p>

<p>Eagleman and his team at the Eagleman Laboratory for Perception and Action at the Stanford University School of Medicine have created <a href="http://www.eagleman.com/research/sensory-substitution">VEST</a> (the Versatile Extra-Sensory Transducer), a wearable <a contenteditable="false" data-primary="VEST (Versatile Extra-Sensory Transducer)" data-type="indexterm" id="v3">&nbsp;</a>device that allows people who are <a contenteditable="false" data-primary="deafness, assistive technology for" data-type="indexterm" id="datf3">&nbsp;</a>deaf to feel speech through a series of vibrations. A <a contenteditable="false" data-primary="smartphones" data-type="indexterm">&nbsp;</a>smartphone or tablet uses an app and a microphone to pick up sounds. It then sends those sounds via Bluetooth to the vest. The vest converts the sounds into vibrations, which are felt on the back of the wearer’s body.</p>

<p>Eagleman makes the important distinction that what the person is feeling isn’t to be interpreted like braille. As he said<span data-type="footnote">Shirley Li, <a href="http://theatln.tc/2hrk6VM">“The Wearable Device That Could Unlock a New Human Sense,”</a> <em>The Atlantic</em>, April 14, 2015.</span> in a 2015 TED talk:<span data-type="footnote">David Eagleman, <a href="http://bit.ly/2hrqVXk">“Can we create new senses for humans?”</a> <em>TED</em>, March, 2015.</span></p>

<blockquote>
<p>The pattern of vibrations that you’re feeling while wearing the vest represent the frequencies that are present in the sound. What you’re feeling is not code for a letter or a word—it’s not like Morse code—but you’re actually feeling a representation of the sound.</p>
</blockquote>

<p>Eagleman distinguishes the vest from wearable devices such as the <a contenteditable="false" data-primary="Apple" data-secondary="Apple Watch" data-type="indexterm">&nbsp;</a>Apple Watch, where different patterns of vibration are assigned to mean different things (for example, one set of vibration patterns for an incoming tweet, and another for an incoming text message).</p>

<p>Instead, the VEST uses “sensory substitution”—information is fed into the brain via unusual sensory channels, and the brain then figures out what to do with it. This is similar to the <a contenteditable="false" data-primary="EyeMusic" data-type="indexterm">&nbsp;</a>EyeMusic and the <a contenteditable="false" data-primary="vOICe software" data-type="indexterm">&nbsp;</a>vOICe products discussed in <a data-type="xref" href="#seeing_the_world_anew">#seeing_the_world_anew</a>, for which the brain is trained to see with sound. In those cases, instead of traveling through the visual cortex, the signal enters the brain through the auditory cortex and is diverted to the proper spot of the brain. The VEST similarly uses sensory substitution for people who are deaf.</p>

<p>Eagleman and his team have been testing the VEST on participants who are deaf. During his TED talk, Eagleman showed documentation of how Jonathan, a 37-year-old male who was born deaf, was translating complicated patterns of vibrations into an understanding of what was being said after training with the device for five days.</p>

<p>The next step for Eagleman and his team in developing the VEST is to include other data streams beyond auditory information, such as stock market data or weather data. For example, if stock market data were converted into buzzing, people wearing the VEST might start to feel like they’re having intuitions about certain economic <a contenteditable="false" data-primary="VEST (Versatile Extra-Sensory Transducer)" data-startref="v3" data-type="indexterm">&nbsp;</a>trends.</p>

<p>“We no longer have to wait for Mother Nature’s sensory gifts on her timescales,” he said. “Instead, like any good parent, she’s given us the tools that we need to go out and define our own trajectory.”</p>

<p>So, the question now is: how will we use our senses, such as touch, to augment our experience of the world in new ways?</p>

<p>In <a data-type="xref" href="#seeing_the_world_anew">#seeing_the_world_anew</a>, we looked at how AR can translate our environment and context in novel ways through new ways of seeing. Here, with Eagleman’s vision for the future, we can take that further to not only see anew, but feel anew, to engage and know our world in unprecedented ways through touch. From the ability to reach into a deformable screen, to empathizing with another individual by feeling vibrations sent to our hands, to a new perceptual understanding of the world through a haptics vest like Eagleman’s, one thing is for certain: our augmented future won’t feel “glassy,” it will be visceral. <a contenteditable="false" data-primary="Eagleman, David" data-startref="ef3" data-type="indexterm">&nbsp;</a>As humans, we move through a 3-D world and feel things with our entire bodies, yet technology often limits us to a 2-D plane. The most powerful AR experiences will enable us to feel things the way we do in reality, and even use these human capabilities in new hybrid digital ways to extend and further augment our <a contenteditable="false" data-primary="haptic technology" data-secondary="sensory substitution" data-startref="ht3sens" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="touch" data-secondary="sensory substitution" data-startref="t3ss" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="sensory substitution" data-startref="ssub3" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="deafness, assistive technology for" data-startref="datf3" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="touch" data-seealso="haptic technology" data-startref="t3" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="haptic technology" data-secondary="VEST (Versatile Extra-Sensory Transducer)" data-startref="ht3v" data-type="indexterm">&nbsp;</a>senses.</p>
</section>
</section>
