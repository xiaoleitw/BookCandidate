<section data-type="chapter" id="seeing_the_world_anew" xmlns="http://www.w3.org/1999/xhtml">
<h1>Seeing the World Anew</h1>

<p>We are at the beginning of a massive change in how we see and experience reality. Computer vision, <a contenteditable="false" data-primary="machine learning" data-type="indexterm">&nbsp;</a>machine learning, new types of cameras, sensors, and wearable devices are extending human perception in extraordinary ways. Augmented Reality (AR) is giving us new eyes.</p>

<p>AR’s evolution as a new communications medium is rooted in the history of the moving image and early cinema. In 1929, pioneering <a contenteditable="false" data-primary="film" data-secondary="Man with a Movie Camera" data-type="indexterm">&nbsp;</a>filmmaker <a contenteditable="false" data-primary="Vertov, Dziga" data-type="indexterm">&nbsp;</a>Dziga Vertov wrote about the power of the camera to depict a new reality, “I am a mechanical eye. I, a machine, show you the world as only I can see it.” Vertov’s famous film <a contenteditable="false" data-primary="Man with a Movie Camera (film)" data-type="indexterm">&nbsp;</a><em>Man with a Movie Camera</em> used innovative camera angles and techniques to defy the limitations of human vision.</p>

<p>Vertov experimented with novel vantage points (such as filming from moving vehicles like a motorcycle, to placing a camera on the train tracks while a train passed overhead). He also explored a new sense of time and space by superimposing images and speeding up and slowing down film. Vertov used the emerging technology of the mechanical camera to extend the capabilities of the human eye and create new ways of seeing. He wrote, “My path leads to the creation of a fresh perspective of the world. I decipher in a new way a world unknown to you.”</p>

<p>Nearly a century later, Vertov’s path has led us to AR revealing a new reality and understanding of our world. The camera plays a central role in how AR technology traditionally works: a camera is paired with computer vision to scan and decipher our physical surroundings. AR previously relied heavily on <a contenteditable="false" data-primary="fiducial markers" data-type="indexterm">&nbsp;</a>fiducial markers (black and white geometric patterns) or images to augment two-dimensional (2-D) surfaces, such as a print magazine.</p>

<p>The real world, however, is not flat; we experience it in three-dimensional (3-D) space. Unlike 2-D fiducial markers or images, 3-D depth-sensing cameras are being used in AR to recognize, map, and understand our spatial surroundings. These 3-D depth-sensing cameras, such as the <a contenteditable="false" data-primary="Kinect" data-type="indexterm" id="k2">&nbsp;</a> Microsoft Kinect camera and Intel’s RealSense <a contenteditable="false" data-primary="Intel RealSense" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="RealSense" data-type="indexterm">&nbsp;</a>camera, are replacing the use of fiducial markers and images to change the way computers see, translate, and augment 3-D environments.</p>

<p>Vertov’s work explored how the camera as a mechanical eye could defy the limits of human vision. He presented novel perspectives depicting what it would be like if a human could see like a camera. Depth-sensing cameras like Kinect and RealSense present the opposite: what if a camera and computer could see like a human? AR technology is beginning to mimic the design of human sensibilities allowing us to see in completely new ways.</p>

<section data-type="sect1" id="you_are_the_controller">
<h1>You Are the Controller</h1>

<p>When introduced in 2010, Kinect changed the way we experienced AR. Kinect’s tag line was “You are the controller.” By simply moving your body as you naturally would, you triggered and directed the AR experience.</p>

<p>Prior to Kinect, for AR to appear on your body, you would need to cover yourself in 2-D fiducial markers, have an image printed on your clothing, or get an AR tattoo. But with Kinect, the experience instantly became more immersive because there was no barrier between you and the augmentation; it was you. Standing in front of a screen powered by Kinect, you could <a href="https://youtu.be/bx5McnEht7Q">see and interact with a transformed version of yourself</a>, as though standing in front of a magical digital mirror. The augmentations followed your movement and responded to your gestures, creating an experience unique to you.</p>

<p>Artists <a contenteditable="false" data-primary="artist" data-type="indexterm" id="art2">&nbsp;</a>immediately embraced Kinect as a creative tool to build new types of interactive experiences. <a contenteditable="false" data-primary="Milk, Chris" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="The Treachery of Sanctuary" data-type="indexterm">&nbsp;</a>Chris Milk’s “<a href="http://milk.co/treachery">The Treachery of Sanctuary</a>” (2012), is a beautiful example of Kinect used in an art installation. You are invited to stand in front of a series of three interactive panels that represent the creative process through birth, death, and regeneration. Your body is mirrored back to you as a dark shadow with different transformations occurring in each panel. In the first panel, your body disintegrates into rising birds. As you move into the second panel, these same birds swoop in to assail you. In the third and final panel, your body sprouts giant wings, and by flapping your arms, your form takes flight, rising off the ground and ascending into the sky.</p>

<p>Milk writes in an artist statement:<span data-type="footnote"><a href="http://milk.co/tos-statement">“The Treachery of Sanctuary.”</a></span></p>

<blockquote>
<p>What is interesting to me is the two-way conversation between the work and the viewer. The participant is an active character in the content and concept of the piece, and while the technology allows that interactivity, the emphasis is on the experience, on transcending past the enabling innovation to the spiritual immersion.</p>
</blockquote>

<p>Part of Kinect’s magic is that the technology becomes invisible because it is easy to use: you stand in front of it and move around. The experience is reactive to you; your body movements trigger what happens. The technology enables the experience, but without you, there is no content. The technology recedes into the background and you become the focus, quite <a contenteditable="false" data-primary="artist" data-startref="art2" data-type="indexterm">&nbsp;</a>literally.</p>
</section>

<section data-type="sect1" id="observing_movement_and_predicting_activi">
<h1>Observing Movement and Predicting Activities</h1>

<p>Kinect uses a depth-sensing camera to see the world in three dimensions. It works by projecting a pattern of infrared light points onto a room and then measuring how long it takes the light from each of those points to reflect back to the camera’s sensor chip. Software processes the data to identify any human shapes that might be in view, like heads or limbs. Kinect uses a skeletal model that breaks down the human body into multiple segments and joints. Programmed with more than 200 poses, the software understands how a human body moves and is able to predict what movement your body is likely to make next.</p>

<p>Prediction is an important aspect of human perception that we use extensively in our daily activities to interact with our surroundings. Jeff Hawkins, the founder of Palm computing—the company that gave us the first handheld computer—and author of the book <em>On Intelligence</em> (Times Books, 2004), describes the human brain as a memory system that stores and plays back experiences to help us predict what will happen next.</p>

<p>Hawkins points out that the human brain is making constant predictions about what is going to happen in our environment. We experience the world through a sequence of patterns that we store and recall, which we then use to match up against reality to anticipate what will happen next.</p>

<p>Using Kinect, researchers at Cornell University’s Personal Robotics Lab <a href="http://pr.cs.cornell.edu/anticipation/index.php">programmed a robot to anticipate human actions</a>, and assist in tasks like pouring a drink or opening a refrigerator door. <a contenteditable="false" data-primary="robots/robotics" data-type="indexterm" id="r2st">&nbsp;</a>The robot observes your body movements to detect what action is currently taking place. It accesses a video database of about 120 household activities (ranging from brushing your teeth, to eating, to putting food in the microwave) to predict what movement you will make next. The robot then plans ahead to assist you in that <a contenteditable="false" data-primary="Kinect" data-startref="k2" data-type="indexterm">&nbsp;</a>&nbsp;task.</p>
</section>

<section data-type="sect1" id="building_a_3-d_map_with_slam_technology">
<h1>Building a 3-D Map with SLAM Technology</h1>

<p>For a <a contenteditable="false" data-primary="SLAM (Simultaneous Localization and Mapping) technology" data-type="indexterm" id="slam2">&nbsp;</a>robot to move through an environment and perform such activities, it needs to be able to create a map of its surroundings and understand its location within it. Roboticists have developed Simultaneous Localization and Mapping (SLAM) technology to accomplish this task. Prior to SLAM the sensors required to build that map have traditionally been expensive and bulky. Kinect introduced an affordable and lightweight solution. Videos of Kinect-enabled robots appeared on YouTube within weeks of Kinect’s release. The robots ranged from a <a href="http://bit.ly/2u8kDSu">quadrotor flying autonomously</a> <a contenteditable="false" data-primary="drones" data-type="indexterm">&nbsp;</a>around a room, to a robot capable of <a href="http://cnet.co/2hqYUzf">navigating rubble to find earthquake survivors</a>.</p>

<p>Google’s <a href="http://www.google.com/selfdrivingcar/how/">self-driving car</a> also uses <a contenteditable="false" data-primary="autonomous vehicles" data-type="indexterm">&nbsp;</a>SLAM technology with its own camera and sensors. The car processes both map and sensor data to determine its location and detects objects around it based on their size, shape, and movement. Software predicts what the objects might do next and the car performs a responsive action such as yielding to a pedestrian crossing the street.</p>

<p>SLAM is not limited to autonomous vehicles, robots, or drones; humans can use it to map their environment, too. MIT developed one of the first examples of a <a href="http://bit.ly/2wb1Q6y">wearable SLAM device</a> for human users. The system was initially designed for emergency personnel like first responders who enter unknown territory. With a Kinect camera worn on the chest, a digital 3-D map is built in real time as the user moves through the environment. Specific locations can be annotated using a handheld pushbutton. The map can be shared and immediately transferred wirelessly to an offsite commander.</p>

<p>SLAM also makes possible new forms of game play. <a href="https://youtu.be/WHGtvdxTVZk"><em>Ball Invasion</em></a>, <a contenteditable="false" data-primary="Ball Invasion" data-type="indexterm">&nbsp;</a>created in 2011 by <a contenteditable="false" data-primary="13th Lab" data-primary-sortas="thirteenth" data-type="indexterm">&nbsp;</a>13th Lab in Stockholm, Sweden, is an early example of integrating SLAM into an AR game. Holding your iPad in front of you, you see your physical surroundings filled with virtual targets to shoot and chase. What made <em>Ball Invasion</em> unique is that the virtual elements interact with your physical world: virtual bullets bounce off the wall in front of you, and virtual invading balls hide behind your furniture. As you play the game and move the iPad’s camera around, you are building a real time 3-D map of the environment to enable these interactions. In 2012, 13th Lab released <a href="https://youtu.be/K5OKaK3Ay8U">PointCloud</a>, <a contenteditable="false" data-primary="PointCloud" data-type="indexterm">&nbsp;</a>an iOS Software Development Kit (SDK) with SLAM 3-D technology for app developers. 13th Lab was acquired by VR technology company Oculus in 2014.</p>

<p>Today, SLAM is one of the underlying technologies behind <a contenteditable="false" data-primary="Google" data-secondary="Tango" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Tango" data-type="indexterm">&nbsp;</a>Google’s Tango AR computing platform. In 2015, tablet development kits for Tango became available to professional developers first, with Tango-enabled <a contenteditable="false" data-primary="smartphones" data-secondary="Tango-enabled" data-type="indexterm">&nbsp;</a>smartphones introduced later in 2016 (the Lenovo Phab 2 Pro) and 2017 (the Asus ZenFone AR). Tango makes possible experiences such as precise navigation without GPS, windows into virtual 3-D worlds, measuring spaces in real time, and games that know where they are in a room and what’s around them. Google describes<span data-type="footnote"><a href="https://www.google.com/atap/project-tango/">Google Tango</a></span> the goal of Tango as giving “mobile devices a human-scale understanding of space and motion.”</p>

<p>Our smartphones are already an extension of ourselves, and with advancements like Tango, smartphones are beginning to see, learn, and understand the world like we do. This will give way to new types of interactions in which the virtual is seamlessly mapped to our physical reality and is contextually relevant, creating a deeper sense of immersion. The lines between the virtual and real will blur even more. Technology will not only understand our surroundings, but perhaps help navigate us through our daily lives with a new-found intelligence and <a contenteditable="false" data-primary="robots/robotics" data-startref="r2st" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="SLAM (Simultaneous Localization and Mapping) technology" data-startref="slam2" data-type="indexterm">&nbsp;</a>awareness.</p>
</section>

<section data-type="sect1" id="helping_the_blind_to_see">
<h1>Helping the Blind to See</h1>

<p>If we can bring vision to computers and tablets, why not use that same technology to help <a contenteditable="false" data-primary="vision assistance applications" data-type="indexterm" id="vaa2">&nbsp;</a>people see? Rajiv Mongia, <a contenteditable="false" data-primary="Mongia, Rajiv" data-type="indexterm">&nbsp;</a>director of the <a contenteditable="false" data-primary="Intel RealSense" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="RealSense" data-type="indexterm">&nbsp;</a>Intel RealSense Interaction Design Group, and his team have developed a portable prototype of a wearable device that uses RealSense 3-D camera technology to help people who are vision-impaired gain a better sense of their surroundings.</p>

<p>Demonstrated live on stage at the 2015 International Consumer Electronics Show (CES) in Las Vegas, the RealSense Spatial Awareness Wearable consists of a vest fitted with a computer that connects wirelessly to eight thumb-sized vibrating sensors worn across the chest, torso, and near the ankles of each leg. It works by seeing depth information to sense the environment around the wearer. Feedback is sent to the wearer through <a contenteditable="false" data-primary="haptic technology" data-type="indexterm">&nbsp;</a>haptic technology that uses vibration motors for tactile feedback.</p>

<p>The vibration sensors are comparable to the vibration mode on your phone with the intensity of the vibration being proportional to how close an object is to you. If an object is very close, the vibration is stronger, and if it’s farther away, it is lower.</p>

<p>Darryl Adams, <a contenteditable="false" data-primary="Adams, Darryl" data-type="indexterm">&nbsp;</a>a technical project manager at Intel, has been testing the system. Adams was diagnosed with retinitis pigmentosa 30 years ago and says the technology allows him to make the most of the vision he does have by augmenting his peripheral vision with the sensation of touch.</p>

<blockquote>
<p>For me, there is tremendous value in the ability to recognize when change occurs in my periphery. If I am standing still and I feel a vibration, I am instantly able to turn in the general direction to see what has changed. This would typically be somebody approaching me, so in this case I can greet them, or at least acknowledge they are there. Without the technology, I typically miss this type of change in my social space so it can often be a bit awkward.</p>
</blockquote>

<p>The system was tested on three wearers, each with very different needs and levels of vision, from low vision to fully blind. Mongia and his team are working on making the system scalable with modular hardware components to allow users to select the combination of sensors and haptic output that best suits their specific situation.</p>

<p>Adams would like to see the software become context-aware so that the system can respond to the wearer’s needs in any given scenario. He thinks this technology could evolve to include features like facial recognition or eye tracking. This way, the wearer can be alerted when someone is looking at them rather than just knowing there is a person nearby.</p>

<p>Artificial Intelligence (AI) <a contenteditable="false" data-primary="Artificial Intelligence (AI)" data-type="indexterm">&nbsp;</a>could further be integrated to provide wearable systems with a better understanding of the wearer’s context. Methods like machine learning can help give computers some of the abilities of a human brain, enabling computer programs to learn to perform new tasks when exposed to new data, without being explicitly programmed for those tasks.</p>
</section>

<section data-type="sect1" id="teaching_a_computer_to_see_with_machine">
<h1>Teaching a Computer to See with Machine Learning</h1>

<p><a href="http://www.orcam.com/">OrCam</a>, a <a contenteditable="false" data-primary="OrCam" data-type="indexterm" id="oc2">&nbsp;</a>device designed for the visually impaired, uses <a contenteditable="false" data-primary="machine learning" data-type="indexterm">&nbsp;</a>machine learning to help wearers interpret and better interact with their physical surroundings. The device can read text and recognize things like faces, products, and paper currency.</p>

<p>The OrCam device features a camera that clips on to a pair of <a contenteditable="false" data-primary="Augmented Reality (AR)" data-secondary="glasses" data-type="indexterm">&nbsp;</a>glasses and continuously scans the wearer’s field of view. The camera is connected by a thin cable to a portable computer that fits in your pocket. Instead of vibration sensors (like in the RealSense Spatial Awareness Wearable), OrCam uses audio. A bone-conduction speaker transmits sound to the wearer as it reads aloud objects, words, or people’s names.</p>

<p>With OrCam, the wearer shows the device what he is interested in by pointing. “Point at a book, the device will read it,” says Yonatan Wexler,<span data-type="footnote">Helen Papagiannis, <a href="https://iq.intel.com/augmented-reality-applications-helping-the-blind-to-see/">“Augmented Reality Applications: Helping the Blind to See,”</a> <em>iQ</em>, February 10, 2015.</span> <a contenteditable="false" data-primary="Wexler, Yonatan" data-type="indexterm">&nbsp;</a>head of research and development at OrCam. “Move your finger along a phone bill, and the device will read the lines letting you figure out who it is from and the amount due.” To teach the system to read, it is repeatedly shown millions of examples, so the algorithms focus on relevant and reliable patterns.</p>

<p>Wexler says there is no need to point when <a contenteditable="false" data-primary="facial recognition" data-type="indexterm">&nbsp;</a>identifying people and faces. “The device will tell you when your friend is approaching you. It takes about ten seconds to teach the device to recognize a person,” he says. “All it takes is having that person look at you and then stating their name.” OrCam takes a photo of the person and stores it within the system’s memory. The next time the camera views the person, the device will recognize that person, and even identify them by name.</p>

<p>OrCam uses machine learning to recognize faces. The research and development team had to provide OrCam with hundreds of thousands of images of all different kinds of faces in order to teach OrCam’s program how to recognize an individual face. When a user is wearing OrCam, the program sorts through all images, rejecting ones that are not a match, until only the one matching picture remains. In a matter of moments, this process of face recognition occurs each time someone wearing OrCam encounters someone they have taken a picture of with their device.</p>
</section>

<section data-type="sect1" id="training_the_brain_to_see_with_sound">
<h1>Training the Brain to See with Sound</h1>

<p>OrCam is <a contenteditable="false" data-primary="sound, seeing with" data-type="indexterm" id="ssw2">&nbsp;</a>trained to <a contenteditable="false" data-primary="OrCam" data-startref="oc2" data-type="indexterm">&nbsp;</a>see your world and provide an oral translation of your immediate surroundings. A different approach is taken by vision technologies like the <a href="https://www.seeingwithsound.com/about.htm">vOICe</a> and <a href="http://apple.co/2u48Xwa">EyeMusic</a>. Instead of using machine learning to tell the wearer what she is looking at, these technologies explore how the human brain can be trained to see with other senses, effectively learning how to see with sound.</p>

<p>Neuroscientist <a contenteditable="false" data-primary="Amedi, Amir" data-type="indexterm">&nbsp;</a>Amir Amedi asks, “What if we found a way to deliver the visual information in the brain of individuals who are visually impaired, bypassing the problems in their eyes?” Brain imaging studies conducted by Amedi and his team show that when people who have been blind since birth use systems like <a contenteditable="false" data-primary="vOICe software" data-type="indexterm">&nbsp;</a>the vOICe and EyeMusic to “see,” they activate the same category-dependent processing areas of the brain as people who are sighted. However, instead of traveling through the visual cortex, the signal enters the brain through the auditory cortex and is diverted to the proper spot of the brain.</p>

<p>The vOICe system (OIC = “Oh, I See”) translates images from a camera into audio signals to assist people who are congenitally blind to see. Developed by <a contenteditable="false" data-primary="Meijer, Peter" data-type="indexterm">&nbsp;</a>Peter Meijer, the vOICe consists of a pair of <a contenteditable="false" data-primary="Augmented Reality (AR)" data-secondary="glasses" data-type="indexterm">&nbsp;</a>sunglasses with a small camera that is connected to a computer and a pair of headphones. (The system also can be used on a smartphone by downloading the software and using the phone’s built-in camera.)</p>

<p>The vOICe software makes your surroundings into a “soundscape.” The camera continuously scans the environment from left to right, converting each pixel into a beep: the frequency represents its vertical position, and the volume of each beep represents the brightness of the pixel. Brighter objects are louder, and frequency indicates whether an object is high or low.</p>

<p>Amedi and his colleagues have trained people who were born blind to “see” using the vOICe and EyeMusic, a <a contenteditable="false" data-primary="EyeMusic" data-type="indexterm">&nbsp;</a>more recent app developed by Amedi that additionally assigns different pitches to colors. Different types of instruments are used to convey colors. For example, blue is signified by a trumpet, red by chords played on an organ, and yellow by a violin. White is represented by human voices, and black is silence.</p>

<p>Amedi says training one’s brain to learn to see this way takes about 70 hours. Users are taught how to identify broad categories of objects, including faces, bodies, and landscapes. Each is processed in the visual cortex of the brain. “Everyone thinks that the brain organizes according to the senses, but our research suggests this is not the case,” says Amedi.<span data-type="footnote">Roni Jacobson, <a href="http://bit.ly/2wa9Btg">“App Helps the Blind ‘See’ With Their Ears,”</a> <em>National Geographic</em>, April 5, 2014.</span> “The human brain is more flexible than we thought.”</p>

<p>Research and inventions like Amedi’s and Meijer’s show us that the traditional definition of what it means to see is changing. It will continue to change as both computers and the human brain are learning to see in new ways <a contenteditable="false" data-primary="sound, seeing with" data-startref="ssw2" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="vision assistance applications" data-startref="vaa2" data-type="indexterm">&nbsp;</a>together.</p>
</section>

<section data-type="sect1" id="choose_your_own_reality">
<h1>Choose Your Own Reality</h1>

<p>The ability to <a contenteditable="false" data-primary="filtering reality" data-type="indexterm" id="fr2">&nbsp;</a>see and interpret our surroundings with the assistance of computer vision also makes it possible to filter our reality and selectively see, or unsee, the world around us. This includes the possibility of removing things from our reality that we do not wish to see.</p>

<p><em>Black Mirror</em>, a <a contenteditable="false" data-primary="Black Mirror" data-type="indexterm">&nbsp;</a>popular television series satirizing modern technology, imagined the ability to block people in real life with the press of a button in the episode “White Christmas” (2014). Instead of seeing the person you have blocked, you now see a white space in the shape of a person, and hear muffled sounds, with the blocked person seeing the same for you. In 2010, Japanese developer <a contenteditable="false" data-primary="Fukatsu, Takayuki" data-type="indexterm">&nbsp;</a>Takayuki Fukatsu built a demonstration not too different from the technology presented in the episode of <em>Black Mirror</em>. Using <a contenteditable="false" data-primary="Kinect" data-type="indexterm">&nbsp;</a>Kinect and <a contenteditable="false" data-primary="OpenFrameworks" data-type="indexterm">&nbsp;</a>OpenFrameworks, Fukatsu’s <a href="https://youtu.be/4qhXQ_1CQjg">Optical Camouflage</a> shows <a contenteditable="false" data-primary="Optical Camouflage" data-type="indexterm">&nbsp;</a>a human figure blending in with his background to become invisible.</p>

<p>Dr. Steve Mann <a contenteditable="false" data-primary="Mann, Steve" data-type="indexterm">&nbsp;</a>is a professor of electrical engineering and computer science at the University of Toronto and is referred to as “the father of wearable computing.” Mann defined the term “Mediated Reality” in the 1990s. He says,<span data-type="footnote">Steve Mann, <a href="http://www.linuxjournal.com/node/3265/print">“Mediated Reality: University of Toronto RWM Project,”</a> <em>Linux Journal</em>, March 1, 1999.</span> “Mediated Reality differs from <a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="differentiated from Mediated Reality" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Mediated Reality" data-type="indexterm" id="mr2">&nbsp;</a>virtual reality (or augmented reality) in the sense that it allows us to filter out things we do not wish to have thrust upon us against our will.” For Mann, wearable computing devices provide the user with “a self-created personal space.” Mann has used Mediated Reality to substitute personal notes and directions in place of advertisements.</p>

<p>New media artist Julian Oliver <a contenteditable="false" data-primary="Oliver, Julian" data-type="indexterm">&nbsp;</a>credits Mann’s work as inspiration for “The Artvertiser,” a <a contenteditable="false" data-primary="The Artvertiser" data-primary-sortas="Artvertiser" data-type="indexterm">&nbsp;</a>Mediated Reality project initiated in 2008, which he developed in collaboration with <a contenteditable="false" data-primary="Stewart, Damian" data-type="indexterm">&nbsp;</a>Damian Stewart and <a contenteditable="false" data-primary="Castro, Arturo" data-type="indexterm">&nbsp;</a>Arturo Castro. <a href="http://theartvertiser.com/">The Artvertiser</a> is a software platform that replaces billboard advertisements with art in real time. It works by teaching computers to recognize advertisements, which are then transformed into a virtual canvas upon which artists can exhibit images or video. The artwork is viewed through a handheld device that looks like a pair of binoculars.</p>

<p>Rather than referring to this as a form of AR technology, Oliver considers The Artvertiser to be an example <a contenteditable="false" data-primary="Improved Reality" data-type="indexterm">&nbsp;</a>of “Improved Reality.” He describes the project as a reclaiming of our public spaces from “read-only” to “read-write” platforms. The Artvertiser applies a subversive approach to reveal and temporarily intercept environments that are dominated by advertising.</p>

<p>The “Brand Killer” (2015) is a contemporary project that builds upon Mann’s and Oliver’s work. Brand Killer was <a contenteditable="false" data-primary="Brand Killer" data-type="indexterm">&nbsp;</a>created by a group of University of Pennsylvania students, <a contenteditable="false" data-primary="Rosenbluth, Reed" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Dubin, Jonathan" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Crits-Christoph, Alex" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Catullo, Tom" data-type="indexterm">&nbsp;</a>Tom Catullo, Alex Crits-Christoph, Jonathan Dubin, and Reed Rosenbluth, to blur ads in real time for its wearer. The students <a href="http://bit.ly/2woxBIC">question</a>, “What if we lived in a world where consumers were blind to the excesses of corporate branding?” Brand Killer is a custom-built head-mounted display that uses openCV image processing to recognize and block brands and logos from the user’s point of view in real time. It’s “AdBlock for Real Life,” they state.</p>

<p>We already mediate our reality while we’re on the internet by blocking ads and even people with whom we no longer want to interact. Beyond advertising and other people, what else will we choose to remove or block from our vision with Mediated Reality?</p>

<p>As we design the future of AR, we will need to consider if digitally filtering, mediating, and substituting content to one’s choosing will enhance our reality or separate us from the world and one another. It is my hope that these new technologies will be used in ways to support human interaction, connection, and communication, and even build empathy.</p>

<p>Although we are often inclined to erase things from reality that we do not want to see, such as homelessness, poverty, and sickness, there are things that we, as society, must actively address. Mediated Reality has the potential to foster a culture of avoidance and even ignorance. We should not turn a blind eye to the realities of reality.</p>

<p>The positive side of Mediated Reality is that it could be used as a way to provide focus. This technology has the possibility to create a future with less distractions that leads to more human-to-human moments. We are already bombarded by technology and notifications; what if Mediated Reality provided an easy way to completely switch off distractions temporarily?</p>

<p>Another critical question is who will be authoring this new reality? Will it be individuals, corporations, or groups of people? Whose Mediated Reality will we be privy to and what visual filters or tools for interception will come to exist? To use Oliver’s terms, will we be part of a read-write environment, or read-only?</p>

<p>In the same way that the internet is read-write, I believe AR, with Mediated Reality a part of it, will be, too. Tim Berners-Lee, inventor of the World Wide Web, envisioned the internet as a place to share experiences in new and powerful ways. “The original thing I wanted to do was make it a collaborative medium, a place where we can all meet and read and write,” he says.<span data-type="footnote">Andy Carvin, <a href="http://bit.ly/2wp2kVT">“Tim Berners-Lee: Weaving a Semantic Web,”</a> February 1, 2005.</span> The internet reframed the way we share and consume information and AR has the power to do <a contenteditable="false" data-primary="Mediated Reality" data-startref="mr2" data-type="indexterm">&nbsp;</a>this, as well.</p>

<p>With examples such as enabling the visually impaired to gain a form of sight, artists imagining new interactive experiences, and robots assisting humans in daily life, AR presents a new way of perceiving the world. AR has the ability to improve people’s lives and inspire life changing ways of engaging with our surroundings and one another.</p>

<p>If we replace the word “machine” with the word “human” in <a contenteditable="false" data-primary="Vertov, Dziga" data-type="indexterm">&nbsp;</a>Vertov’s sentiment at the beginning of this chapter, “I, a machine, show you the world as only I can see it,” we get the richness of what the internet enables: a global collection of shared stories of human experiences and perspectives. To have a positive impact on society and contribute to humanity in a meaningful way, AR will need to find ways to mirror the original vision for the World Wide Web to largely be inclusive, not <a contenteditable="false" data-primary="filtering reality" data-startref="fr2" data-type="indexterm">&nbsp;</a>exclusive.</p>
</section>
</section>
