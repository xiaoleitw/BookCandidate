<section data-type="chapter" id="digital_smell_and_taste" xmlns="http://www.w3.org/1999/xhtml">
<h1>Digital Smell and Taste</h1>

<p>On April 1, 2013, <a contenteditable="false" data-primary="taste" data-see="smell and taste, digital" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="digital smell and taste" data-see="smell and taste, digital" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="smell and taste, digital" data-type="indexterm" id="satd5">&nbsp;</a>Google announced “Google Nose,” a <a contenteditable="false" data-primary="Google" data-secondary="Google Nose" data-type="indexterm">&nbsp;</a>way to digitally capture and search for smells. In a company <a href="https://youtu.be/9-P6jEMtixY">video</a>, product manager Jon Wooley pointed out how smell was an important part of the search experience that Google had previously overlooked. Google Nose would allow you to go beyond type, speech, and touch to retrieve information and gain knowledge about the world. Powered by the Google Aromabase database of 15 million scentibytes from around the world, Google Nose would identify a particular smell in your environment, or emit aroma results from a keyword search.</p>

<p>Google Nose was an April Fool’s Day prank and not a real product. But digital scent experienced over the internet isn’t that far-fetched. In fact, it’s happening today to help make people’s lives better via hardware and wearables that release beneficial aromas for adults suffering from Dementia and Alzheimer’s. It is even being used as an early diagnostic tool.</p>

<p>The human sensorium is not limited to vision, touch, and sound. If we are to engage Augmented Reality (AR) with all of our senses, we can’t forget smell and taste. They are the only two senses that are directly connected to the limbic system of the brain, which is responsible for emotion and memory. Smell and taste can be incredibly personal carriers of stories, memories, and emotions. These two senses can transport you to the past, or bring your attention to the present.</p>

<p>The field of digital smell and taste is a growing area of research, prototypes, and product design aiming to expand the way we perceive and interact with our surroundings. In this chapter, we look at how new wearable technologies and interfaces that focus on virtual smell and taste can augment the way we share and receive information, enhance entertainment experiences, deepen our understanding of a place, and affect our overall wellbeing.</p>

<section data-type="sect1" id="smell-o-vision_returns">
<h1>Smell-O-Vision Returns</h1>

<p>The idea of enhancing a <a contenteditable="false" data-primary="smell and taste, digital" data-secondary="Smell-O-Vision" data-type="indexterm" id="satd5sov">&nbsp;</a>&nbsp;media experience with smell has its roots in Smell-O-Vision, invented <a contenteditable="false" data-primary="Smell-O-Vision" data-type="indexterm">&nbsp;</a>by Hans Laube, which debuted at the 1939 New York World’s Fair. In 1960, the technology was brought to movie theatres, with the &nbsp;first Smell-O-Vision film, <a contenteditable="false" data-primary="Scent of Mystery (film)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="film" data-secondary="Scent of Mystery" data-type="indexterm">&nbsp;</a><em>Scent of Mystery</em>. An automated scent track was pumped directly to individual seats via plastic tubing, with aromas synchronized to actions on the screen. The thirty different scents included the perfume of the mystery girl in the film, tobacco, orange, shoe polish, wine (when a character was crushed to death by falling casks), baked bread, coffee, and peppermint. An advertisement<span data-type="footnote"><a href="http://bit.ly/2wb9jST">“Scent of Mystery with Smell-O-Vision Powered by Scentevents,”</a> October 13, 2015.</span> for the film read, “First they moved (1895)! Then they talked (1927)! Now they smell (1960)!” AR follows a similar technological progression to the history of cinema: beginning with moving silent pictures, then integrating sound, and then experimenting with re-creating other senses like smell.</p>

<p>Between some of the aromas being delayed, the loud distracting hissing noises the scent distribution system made, and certain smells making viewers nauseous, <em>Scent of Mystery</em> was a flop. Plans to bring Smell-O-Vision to other cinemas were halted and the film was rereleased with an odorless version under the title <em>Holiday in Spain</em>.</p>

<p>More than 50 years after its first release, <em>Scent of Mystery</em> was <a href="http://artandolfaction.com/projects/scent-of-mystery/">screened again</a> in October 2015 in Bradford, England, and Copenhagen, Denmark, the way it was originally intended, with smell. “I want this to be a way to bring to people’s attention the potential of smell,” said Tamara Burnstock, the producer of the rescreening. “It’s an opportunity to revive an interest in scented cinema.”</p>

<p>It was also an opportunity to learn what can be done differently today and what lessons can be applied to future augmented-smell experiences. The rescreening reimagined the original scents used for the film, and emphasized audience interaction with analog methods. A fan infused with the “scent of mystery” was placed on each seat for viewers to wave when prompted, spray bottles were released at specific moments, and heavily scented actors walked through the auditorium.</p>

<p>In addition to smell diffusing methods, the rescreening experimented with scent sequences and a stylistic language of smells, which could contribute to a playbook of augmented smell techniques.</p>

<p>For instance, the order in which the scents were released was found to be crucial; the smell of rose had to be introduced before garlic. When it came to representation, some smells, like an orange grove for example, had a direct correlation to what appeared on the screen, whereas other scenes applied smell as base notes to create an atmosphere similar to the way a soundtrack works.</p>

<p>Biophysicist <a contenteditable="false" data-primary="Turin, Luca" data-type="indexterm">&nbsp;</a>Luca Turin, author of <em>The Secret of Scent: Adventures in Perfume and the Science of Smell</em> (HarperCollins, 2006) believes one of the reasons Smell-O-Vision never took off was because you can’t create a large palate of smells the way you can with primary colors. <a contenteditable="false" data-primary="Wilson-Brown, Saskia" data-type="indexterm">&nbsp;</a>Saskia Wilson-Brown, founder of The Institute for Art and Olfaction oversaw the composition and production of the scents for the 2015 screenings, and attested to the challenges of working with smell. Although there is more technology today, and a better understanding of how smell can be used, she said scent is still entirely experimental.</p>

<p>Critics called Smell-O-Vision a gimmick in the 1960s. AR could also be in danger of becoming a gimmick if the focus remains on the technology rather than on delivering an impactful and compelling experience.</p>

<p>“How do you make scent a part of the story without being gimmicky?” Wilson-Brown asked. “It’s a question of semantics and creating a common language and common meaning, which is devilishly hard, because scent can be so personal.”</p>

<p>Yet, maybe it’s not as much about generating a common language or a common meaning of scent; perhaps the real opportunity with augmented smell is in personalization. Both the original screening and the rescreening of <em>Scent of Mystery</em> with Smell-O-Vision were to a large audience in a public space, which is more difficult to manage on a large scale. Personal or smaller group experiences, with new devices being developed today, point to the future of augmented smell.</p>

<p>Smell-O-Vision was one of the precursors of digitally immersive experiences, such as contemporary devices <a contenteditable="false" data-primary="Nirvana VR helmet" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Feelreal Virtual Reality (VR) mask" data-type="indexterm">&nbsp;</a>like <a href="http://feelreal.com/">Feelreal’s Virtual Reality (VR) mask and Nirvana VR helmet</a>.<span data-type="footnote" id="fn05">The prototype was demonstrated at The Game Developers Conference (GDC) 2015 in San Francisco, the largest annual gathering of professional video game developers. FeelReal announced its KickStarter campaign after GDC; however, the campaign was not successful: $24,568 was pledged of the $50,000 goal. The mask and helmet are listed as available for preorder on the company’s website.</span> These <a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="and smell" data-secondary-sortas="smell" data-type="indexterm">&nbsp;</a>prototypes create sensations through smell as well as wind, heat, water mist, and vibration for three-dimensional (3-D) video games and movies. A mask fits over the lower half of your face and attaches to the Oculus Rift VR headset, whereas a helmet version fits over your head and works with a smartphone display. Both models connect wirelessly via Bluetooth to the digital entertainment source device. An odor generator with seven removable smell cartridges vaporizes scents into your nose. The basic set of smells includes aromas of burning rubber, gunpowder, fire, flowers, jungle, ocean, and aphrodisiacs.</p>

<p>Game developers <a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="for gaming" data-secondary-sortas="gaming" data-type="indexterm">&nbsp;</a>can use the Feelreal SDK to add different smells and effects to create immersive VR games. Smells and effects also can be added to movies without any programming skills using the Feelreal Player. Although Feelreal’s products are currently targeted at VR markets, a more discrete AR system could be designed to also simulate scent for gaming and entertainment experiences.</p>
</section>

<section data-type="sect1" id="personalized_scent_communication_and_sme">
<h1>Personalized Scent Communication and Smell Narratives</h1>

<p>Personalization <a contenteditable="false" data-primary="scent communication" data-seealso="smell and taste, digital" data-type="indexterm" id="scom5">&nbsp;</a><a contenteditable="false" data-primary="smell and taste, digital" data-secondary="personalized scent communication" data-type="indexterm" id="satd5psc">&nbsp;</a>doesn’t mean augmented smell needs to be an isolating or single-user experience. Digital olfactory devices <a contenteditable="false" data-primary="Scentee" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="oNotes" data-type="indexterm">&nbsp;</a>like <a href="http://www.onotes.com/">oNotes</a> and <a href="https://scentee.com/">Scentee</a> enable you to send and receive scent messages using your <a contenteditable="false" data-primary="smartphones" data-secondary="scent and" data-type="indexterm">&nbsp;</a>smartphone. oNotes, developed by Vapor Communications (a startup formed by Harvard professor David Edwards with his former student Rachel Field in 2013), works <a contenteditable="false" data-primary="Field, Rachel" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Edwards, David" data-type="indexterm" id="ed5">&nbsp;</a>by taking a photograph, tagging scents in a mobile app (as you would with hashtags in an Instagram photo), and then sharing it with a friend.</p>

<p>By adding sensory information to an image, we begin to get closer to capturing and re-creating an experience in its totality. Food photography will take on a new dimension: whether you’re tweeting a meal from a new restaurant, or sharing your grandmother’s recipe, your photos can be even more mouth-watering when accompanied by scent. Scent messages could also be used as ways to communicate when words are not sufficient to share a sentiment.</p>

<p>There are 32 scents available in the oNotes app that can be combined to create more than 300,000 scent messages. The photo, accompanied by the primary and secondary notes tagged in the photograph, is received via a hardware device called &nbsp;the oPhone. The oPhone has two cylindrical towers, consisting of oChips cartridges, to create the aromas and release scents.</p>

<p>Edwards hopes oNotes will make possible a new era of sensory experiences in which smell is as integral to the way we consume media as sight and sound are. He is working to create scent narratives and has collaborated with companies like Melchar Media to produce ebooks that use the oPhone to augment storytelling.</p>

<p>The first <a contenteditable="false" data-primary="oBooks" data-type="indexterm">&nbsp;</a>of these “oBooks,” <em>Goldilocks and the Three Bears: The Smelly Version</em>, was exhibited and smelled in 2015 at the Museum of the Moving Image in Queens, New York, and at the Phi Centre in Montreal, Canada. The oBook combines an illustrated children’s ebook on the iPad with the oPhone smell-release system. An emblem with a cartoon nose appears on the screen of selected pages in the ebook, instructing readers to tap it. The nose then disappears and is replaced with the message “Preparing oPhone.” The oPhone, which connects to the iPad via Bluetooth, emits a light puff of air from one of its cylinders with a smell corresponding to the section of the story. The smell lasts for 10 seconds.</p>

<p>Edwards is also working on bringing scent to music with oMusic, an original sound and scent collaboration by composer Daniel Peter Biro and master perfumer Christophe Laudamiel. Replacing words, images, or even music notes with smell can enable new modes of enriched sensorial storytelling with the potential to create a powerfully transformative and emotional experience beyond just sight and <a contenteditable="false" data-primary="Edwards, David" data-startref="ed5" data-type="indexterm">&nbsp;</a>sound.</p>

<p>The oPhone (in its last iteration) wasn’t a very portable device. Digital olfactory devices will have a real chance of reaching a mass audience when the technology is small enough to fit in your pocket, or when it can be directly embedded in your smartphone<a contenteditable="false" data-primary="smartphones" data-seealso="iPhone" data-type="indexterm">&nbsp;</a>, or discretely integrated into other wearable devices like <a contenteditable="false" data-primary="Augmented Reality (AR)" data-secondary="glasses" data-type="indexterm">&nbsp;</a>AR glasses.</p>

<p>Smell can be used as a nonverbal method of communication, assigning meaning to different scents (similar to configuring a library of secret messages through touch with products like <a contenteditable="false" data-primary="Smartstones" data-type="indexterm">&nbsp;</a>Smartstones, as discussed in <a data-type="xref" href="#tactile_sensations">#tactile_sensations</a>). Scentee, <a contenteditable="false" data-primary="Scentee" data-type="indexterm">&nbsp;</a>also developed in 2013, is a portable device shaped like a small bubble (about the size of a cherry tomato) with an LED light that you plug in to the headphone jack of your smartphone. &nbsp;It works with a companion app to spray a burst of fragrance to communicate with friends and family. You can use Scentee to further <a contenteditable="false" data-primary="customizable experiences" data-type="indexterm">&nbsp;</a>customize your experience to receive scent notifications for emails and Facebook “Likes,” and even timer scents that work with your smartphone’s alarm clock. Scentee also offers an SDK for developers to create applications.</p>

<p><a href="http://www.ollyfactory.com/">Olly The Smelly Robot</a>, created by <a contenteditable="false" data-primary="Olly the Smelly Robot" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="robots/robotics" data-type="indexterm">&nbsp;</a>London and New York–based company Mint Foundry, is a USB-powered smell device that also releases a scent when you receive a social media mention, or any other assigned notification. You can <a href="http://www.ollyfactory.com/instructions/">build</a> Olly yourself using a 3-D printer and off-the-shelf parts.</p>

<p>Notifications via augmented scent with devices like Scentee and Olly could present a novel user experience in the future. Instead of text notifications popping up in your AR eyewear, scent could be used to signal an alert. You might want to be careful of what scent you choose for what notification; it could become imprinted on your memory. Choosing a bad smell (which is subjective and unique to each user), however, could also be a way to create a scale of urgency for notifications that are not to be ignored.</p>

<p>I envision a scent cache clearing mode being required for such interfaces. One of the smell lessons learned from the contemporary rescreening of <a contenteditable="false" data-primary="Scent of Mystery (film)" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="film" data-secondary="Scent of Mystery" data-type="indexterm">&nbsp;</a><em>Scent of Mystery</em> was that the scent of a rose should not follow the smell of garlic. In the same way that you sniff a bowl of coffee beans in between smelling multiple perfumes to clear the scent, or a sorbet is served in between meal courses as a palate cleanser, such sense-neutralizing techniques could be applied in augmented scent <a contenteditable="false" data-primary="scent communication" data-seealso="smell and taste, digital" data-startref="scom5" data-type="indexterm">&nbsp;</a>experiences.</p>
</section>

<section data-type="sect1" id="digital_smellscapes">
<h1>Digital Smellscapes</h1>

<p>Augmented <a contenteditable="false" data-primary="smell and taste, digital" data-secondary="smellscapes, digital" data-type="indexterm" id="satd5ds">&nbsp;</a>&nbsp;smell can be used to depict a landscape by creating a smellscape. Archeologist and designer <a contenteditable="false" data-primary="Eve, Stuart" data-type="indexterm" id="es5">&nbsp;</a>Stuart Eve at University College London has developed a prototype to help smell history. The Dead Man’s Nose<span data-type="footnote"><a href="http://bit.ly/2waT9sP">“Archaeology, GIS and Smell (and Arduinos).”</a></span> is <a contenteditable="false" data-primary="The Dead Man's Nose" data-primary-sortas="Dead" data-type="indexterm" id="dmn5">&nbsp;</a>an outdoor AR smell delivery system to transport users back in time to the Bronze Age. It creates smellscapes based on your location through GPS data. Scent is used as a way to further enhance the AR experience, working in combination with the other senses.</p>

<p>There have been several historical recreations executed in AR with visuals, but Eve’s is the first that applies smell as part of the experience. Eve’s AR experience takes place in Cornwall on the site of a prehistoric settlement. He explains,<span data-type="footnote">Ibid.</span> “Not only can I walk around the modern-day Bronze Age landscape and see the augmented roundhouses, hear the Bronze-Age sheep in the distance, I can also smell the fires burning and the dinner cooking as I get closer to the village.”</p>

<p>The Dead Man’s Nose consists of an Arduino board connected to four small computer blowers (fans). These fans are mounted within specially constructed wooden boxes, each of which has a small drawer that contains a piece of cotton wool saturated in a liquid smell. Eve has written a small Arduino sketch that accepts a Bluetooth Low Energy (BLE)<a contenteditable="false" data-type="indexterm" data-primary="iPhone" data-secondary="BLE technology">&nbsp;</a> connection and listens for a coded signal on the serial port. Depending on the signal received, it will either turn on, or off, one of the fans (by sending power to it).</p>

<p>Eve has also written a simple iOS application. The app presents the user with four switches (one for each fan) and a connect button. When the “connect” button is pressed, the app connects via Bluetooth to The Dead Man’s Nose hardware, which then enables the switches. At this point the user can turn any or all of the fans on by simply switching the switches. When one is switched, the appropriate signal is sent via the Bluetooth serial connection, which starts the desired fan.</p>

<p>To create the geo-aspect of The Dead Man’s Nose, the app reads the user’s physical location, via the <a contenteditable="false" data-primary="smartphones" data-secondary="scent and" data-type="indexterm">&nbsp;</a>smartphone’s GPS. Using the inbuilt iOS CoreLocation CircularRegion methods, it creates a number of “geofences” (or smellzones) with a specified radius around a presupplied list of coordinates. Each one of these smellzones is configured with the ID of one or more of the fans, depending on the smells the user wants to smell. When the user physically walks into one of these smellzones, the crossing of the geofence is detected, the serial signal is sent, and the relevant fans begin to spin. Although not yet implemented where there is no GPS coverage (such as inside a gallery), iBeacons (as detailed in <a data-type="xref" href="#audio_and_hearables">#audio_and_hearables</a> with Detour and Cities Unlocked) can also be used to trigger the smells.</p>

<p>All of the smells used in The Dead Man’s Nose are from scent supplier <a href="http://www.daleair.com/">Dale Air</a>. Dale Air has created more than 300 different types of smells ranging from Chicken to Dirty Linen to a scent called Dragon’s Breath. Eve notes the difficulty in choosing the appropriate scent for each of the smellzones:</p>

<blockquote>
<p>As far as I have found no one has yet created the “smell of a cist grave after a ritual.” However, it is important to remember that by recreating the smells of the past we are not necessarily experiencing them or interpreting them as past people would have done, instead we are using them as an aid to challenge the way we think about a site or <a contenteditable="false" data-primary="The Dead Man's Nose" data-primary-sortas="Dead" data-startref="dmn5" data-type="indexterm">&nbsp;</a>landscape.</p>
</blockquote>

<p>Kate McLean is <a contenteditable="false" data-primary="McLean, Kate" data-type="indexterm">&nbsp;</a>another researcher using smell to change the way we think about place, specifically urban landscapes. She <a href="http://sensorymaps.com/about/">says</a>, “I focus on human perception of the urban smellscape. While the visual dominates in data representation I believe we should tap into alternative sensory modes for individual and shared interpretation of place.” McLean’s work on smell focuses on not only what she believes is a neglected human sense, but also a neglected aspect of urban design.</p>

<p>McLean <a contenteditable="false" data-primary="smell and taste, digital" data-secondary="smell walks" data-type="indexterm">&nbsp;</a>has been leading groups on “smell walks” for years across cities such as Amsterdam, Paris, and New York, where she asks locals to wander around their city and make note of their smell impressions. She then uses this smell data to create smell visualizations in the form of city smell maps. She is working on a Smellscaper App to better support the documentation of smells on these walks, including geo-spatially recorded smell notes and enabling participants to follow a smell walk through a city (McLean is not working on emitting scents from the app at this time).</p>

<p>We can think of McLean’s Smellscaper App like the location-aware audio tours by Detour discussed in <a data-type="xref" href="#audio_and_hearables">#audio_and_hearables</a>. Instead of audio, here your nose leads the way, refocusing your senses on surrounding smells and asking you to rethink how you experience a city. The Google Nose April Fool’s joke could in fact be applied to preview a city’s smellmap with a smell-enhanced Google Maps <a contenteditable="false" data-primary="smell and taste, digital" data-secondary="smellscapes, digital" data-startref="satd5ds" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Eve, Stuart" data-startref="es5" data-type="indexterm">&nbsp;</a> or Google Street <a contenteditable="false" data-type="indexterm" data-primary="Google" data-secondary="Google Street View">&nbsp;</a>View.</p>
</section>

<section data-type="sect1" id="health_and_augmented_smell">
<h1>Health and Augmented Smell</h1>

<p>While <a contenteditable="false" data-primary="health" data-type="indexterm" id="health6">&nbsp;</a>urban <a contenteditable="false" data-primary="smell and taste, digital" data-secondary="health applications for" data-type="indexterm" id="satd5haf">&nbsp;</a>landscapes may naturally emit their own scents, products like <a contenteditable="false" data-primary="Ode" data-type="indexterm">&nbsp;</a><a href="http://www.myode.org">Ode</a> are designed to release food fragrances to help adults living with <a contenteditable="false" data-primary="Alzheimer's" data-type="indexterm" id="alz5">&nbsp;</a><a contenteditable="false" data-primary="Dementia" data-type="indexterm" id="dem5">&nbsp;</a>Dementia and Alzheimer’s. With different smells at breakfast, lunch, and dinner, Ode creates a sensory connection with mealtimes, stimulating appetite and helping in situations of weight loss. Ode is a hardware device developed by London-based design agency Rodd, in partnership with fragrance expert <a href="http://www.odettetoilette.com/">Lizzie Ostrom</a> of <a contenteditable="false" data-primary="Ostrom, Lizzie" data-type="indexterm">&nbsp;</a>The Olfactory Experience. It uses smell cartridges that are triggered to come on for a two-hour window around each mealtime.</p>

<p>Weight loss is common among people with late-stage Dementia and can be an early indicator of the condition’s onset. Ode can subliminally promote hunger with pleasant evocative food scents, such as vegetable soup, braised beef casserole, and black forest cake. It can help to improve mood as an additional effect. The company highlights specific behavioral changes from various customers on their website and shares that 50 percent of participants gained an average of 2 kg<span data-type="footnote"><a href="http://www.myode.org/impact-of-ode/">Ode</a></span> in the 11 weeks after Ode was installed. Ode is an example of augmenting an environment with scent to change habits and promote wellbeing.</p>

<p>Whereas Ode scents an entire room with food <a contenteditable="false" data-primary="Tillotson, Jenny" data-type="indexterm">&nbsp;</a>aromas, Dr. Jenny Tillotson is developing eScent<sup>®</sup>, a smell <a contenteditable="false" data-primary="eScent" data-type="indexterm">&nbsp;</a>wearables project that is more localized, delivering smell directly to the user’s nose. eScent forms a noninvasive “scent bubble” around the face, creating an area of constant, detectable scent for the user based on a biometric sensor, sounds, timer, or other sensor-based triggers, as a preprogrammed application on a <a contenteditable="false" data-primary="smartphones" data-secondary="scent and" data-type="indexterm">&nbsp;</a>smartphone. It is a way of emitting microdoses of mood-enhancing or other beneficial aromas at the right time, in the right place, depending on context. This could be the release of wellbeing scents when stress levels are detected, or sleep is disturbed. It could deliver peppermint at the right time to enhance cognitive performance or running speed, or release insect repellent in response to the sound of mosquitoes.</p>

<p>Tillotson believes the real value of eScent is as a diagnosing tool. She explains how eScent can be used by doctors for neurodegeneration purposes by preprogramming a personalized timed-release scent system for a patient using a simple app on a smartphone. “At the moment, doctors diagnose these diseases by observing the sense of smell (or loss of it) manually, so a programme in this way could monitor smelling capabilities in a more accurate way,” says Tillotson.</p>

<p>Tillotson has been talking to neuroscientists and Alzheimer’s experts about this application. She says the difficulty with any test for early Alzheimer’s or Parkinson’s <a contenteditable="false" data-primary="Parkinson's disease" data-type="indexterm">&nbsp;</a>is one of specificity, that is, identifying false positives who have <a contenteditable="false" data-primary="anosmia" data-type="indexterm">&nbsp;</a>anosmia (losing the sense of smell) from other causes. Tillotson is working on a solution for this too. eScent shows us that smell technology is not limited to entertainment, and communication but can have a real impact on the healthcare industry, as <a contenteditable="false" data-primary="health" data-startref="health6" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Alzheimer's" data-startref="alz5" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Dementia" data-startref="dem5" data-type="indexterm">&nbsp;</a>well.</p>
</section>

<section data-type="sect1" id="tasting_the_digital">
<h1>Tasting the Digital</h1>

<p>The <a contenteditable="false" data-primary="smell and taste, digital" data-secondary="tasting" data-type="indexterm" id="satd5t">&nbsp;</a>ability to taste the digital might provide further &nbsp;<a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="health applications for" data-type="indexterm" id="vr5haf">&nbsp;</a>health benefits. <a href="http://www.projectnourished.com/">Project Nourished</a> is a <a contenteditable="false" data-primary="Project Nourished" data-type="indexterm" id="pn5">&nbsp;</a>simulated dining experience in VR to help people with food allergies or intolerances avoid the associated consequences. Jinsoo An, <a contenteditable="false" data-primary="An, Jinsoo" data-type="indexterm">&nbsp;</a>founder of Project Nourished, says he isn’t trying to change our eating habits, given that these food simulations aren’t substitutes for the real thing; rather, he is presenting a new way to occasionally eat the foods that are considered unhealthy, or are restrictive in some diets. The project was inspired by An’s stepfather, who is diabetic, and can no longer eat some of his favorite foods. An wanted to provide a savory simulation without causing a spike in blood sugar.</p>

<p>The system comprises a <a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="and smell" data-secondary-sortas="smell" data-type="indexterm" id="vr5as">&nbsp;</a>VR headset to simulate vision, an aromatic diffuser for producing smell, and a bone conduction transducer for creating chewing sounds through vibration. A gyroscopic utensil is used to manipulate virtual and physical food, and a 3-D printed cube of algae and hydrocolloid polymers add taste and texture. Within the virtual environment, the cubes can take on smell, taste, and texture profiles of the food they seek to replicate. For example, yeast and shiitake mushroom powder are used to recreate dry-aged flavors in a simulated steak. With the primary food component of Project Nourished being algae, An sees another benefit: relying less on resources and reducing our carbon footprint.</p>

<p>Project Nourished is a VR experience; however, an older project from the University of Tokyo called <a href="https://youtu.be/3GnQE9cCf84">Meta Cookie</a> (2010) uses <a contenteditable="false" data-primary="Meta Cookie" data-type="indexterm">&nbsp;</a>AR to also alter your perception of what you’re eating. Meta Cookie combines an interactive olfactory display with plain edible cookies that have AR tracking targets printed on them. A see-through head-mounted display allows the user to view various cookie selections in AR (with different cookie textures and colors visually layered atop the actual cookie). After you select the flavor of cookie that you would like to eat, an air pump produces a scent of the chosen cookie to your nose. This creates the effect that you are eating a flavored cookie, even though it is really a plain one. If you don’t like the taste of the cookie you chose, you can transform it into another flavor and take another bite. In fact, you could have one ultimate cookie that embodies a different flavor with each bite; your experience is entirely <a contenteditable="false" data-primary="customizable experiences" data-type="indexterm">&nbsp;</a>customizable. It’s worth noting, however, that as soon as you eat part of the AR symbol that is printed on the cookie, it ceases to work.</p>

<p>A couple of years later, in 2012, the same research group who created Meta Cookie developed Augmented Satiety to <a contenteditable="false" data-primary="Augmented Satiety" data-type="indexterm">&nbsp;</a>further fool your eyes and taste buds with the intention to make you eat less. By using AR to visually change the perceived size of food, Augmented Satiety is presented as a possible method for decreasing rates of obesity. The researchers point to psychological studies<span data-type="footnote">Takuji Narumi, Yuki Ban, Takashi Kajinami, Tomohiro Tanikawa, Michitaka Hirose, <a href="http://bit.ly/2f905me">“Augmented perception of satiety: controlling food consumption by changing apparent size of food with augmented reality,”</a> <em>CHI '12 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (2012): 109-118.</span> revealing that the amount of food consumed is influenced by both its actual volume and external factors during eating. Based on this knowledge, the researchers sought to control the perception of satiety (the state of being satisfactory full) gained from the same amount of food by changing its apparent size. The researchers’ studies suggest that this augmentation can control the perception of satiety and food intake. Although no clinical trials have been planned for Augmented Satiety, as soon as AR eyewear and smell wearables become pervasive, such eating methods could become a part of daily life.</p>

<p>Perceptions around eating are a reality for people suffering from trauma to the brain. Brain injuries can result in a jumbled sense of texture and flavor when you’re eating, as well as problems translating scale, where your senses might convince you that something is larger than it really is. <a href="http://guerillascience.org/event/brain-banquet/">Brain Banquet</a> was an <a contenteditable="false" data-primary="Brain Banquet" data-type="indexterm">&nbsp;</a>event held in London in March 2014. There, food courses were prepared and served in a way that was totally unrecognizable to attendees. It presented the indescribable feeling that brain trauma victims have where familiar food is experienced in a strange and unfamiliar way. Although they are each distinct investigations and separate from the work presented at the Brain Banquet, both Augmented Satiety and Project Nourished could be further explored as ways to potentially help people suffering from brain injuries, and even as an empathy mechanism to help others understand how such injuries alter your perception of the <a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="health applications for" data-startref="vr5haf" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Project Nourished" data-startref="pn5" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="smell and taste, digital" data-secondary="tasting" data-startref="satd5t" data-type="indexterm">&nbsp;</a>world.</p>
</section>

<section data-type="sect1" id="the_future_of_digital_taste_and_smell">
<h1>The Future of Digital Taste and Smell</h1>

<p>For AR researcher <a contenteditable="false" data-primary="Cheok, Adrian David" data-type="indexterm">&nbsp;</a>Adrian David Cheok, the <a contenteditable="false" data-primary="smell and taste, digital" data-secondary="future applications for" data-type="indexterm" id="satd5faf">&nbsp;</a>future of digital taste is heavily linked to the human brain. Cheok’s previous research inspired smell devices like the Scentee; however, instead of pumping artificial scents to the user’s nose, his current work explores directly stimulating the brain to recreate smell and taste.</p>

<p>Cheok and his team at the Imagineering Institute, a research lab in Malaysia, are developing a device called Digital Taste Interface, <a contenteditable="false" data-primary="Digital Taste Interface" data-type="indexterm">&nbsp;</a>which consists of a plexiglass box that you stick your tongue into to taste different flavors over the internet. Using electrical and thermal stimulation, the interface temporarily tricks your taste receptors into experiencing sour, sweet, bitter, and salty tastes, depending on the frequency of the current passing though the electrodes. Cheok is also designing a system similar to how the Digital Taste Interface works, but for smell. The device has a tiny electrode that one inserts into the nasal cavity, where there are olfactory neurons. The device is still in production.</p>

<p>“Once we can directly stimulate brain neurons, we can bypass the sensors of our body,” said Cheok. “There will be no effective need for stimulating sensors directly. We can already stimulate the neurons in simple ways in humans, so I expect it will be even more sophisticated in the future.” Cheok believes we will see a direct brain interface in our lifetime.</p>

<p>In <a data-type="xref" href="#seeing_the_world_anew">#seeing_the_world_anew</a>, neuroscientist <a contenteditable="false" data-primary="Amedi, Amir" data-type="indexterm">&nbsp;</a>Amir Amedi discussed a way to deliver visual information to the brains of individuals who are visually impaired, bypassing the problems in their eyes. In <a data-type="xref" href="#tactile_sensations">#tactile_sensations</a>, neuroscientist David Eagleman presented a <a contenteditable="false" data-primary="Eagleman, David" data-type="indexterm">&nbsp;</a>future in which the human sensorium is expanded by feeding real time data from the internet directly into your brain and being able to then intuitively experience and feel that data without analyzing it. What new experiences will be made possible when we are able to bypass the nose and tongue, and directly stimulate the brain to create smell and taste?</p>

<p>“In all media, people want to re-create the real world,” says Cheok.<span data-type="footnote"><a href="http://www.theneweconomy.com/technology/using-mobiles-to-smell-how-technology-is-giving-us-our-senses-video">“Using mobiles to smell: how technology is giving us our senses,”</a> <em>The New Economy</em>, February 11, 2014.</span> He explains how when cinema first came out, people were filming city streets. “To be able to capture that on <a contenteditable="false" data-primary="film" data-type="indexterm">&nbsp;</a>film was quite amazing,” Cheok continues, “but as the media developed, then it became a new kind of expression.” He believes it will be the same for taste and smell. Now that digital smell has been introduced, Cheok says at first people wanted to re-create smell at a distance, like sending someone virtual roses with the virtual smell of roses via a <a contenteditable="false" data-primary="smartphones" data-secondary="scent and" data-type="indexterm">&nbsp;</a>smartphone, but he thinks the next stage will lead to new kinds of creations.</p>

<p>We will see novel smell and taste explorations that push beyond literal representations, and even present novel scents and flavors that are not possible in ordinary reality. The <a href="https://youtu.be/9vLSuLL9xLA">Synesthesia Mask</a> by Zachary Howard <a contenteditable="false" data-primary="Howard, Zachary" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Synthesia Mask" data-type="indexterm">&nbsp;</a>allows you to smell colors, and is one such example, assigning scents to colors rather than objects. Although the Synesthesia Mask uses fragrances pumped to the nose (not direct brain stimulation), it begins to experiment with alternate ways to create a new reality that does not have an obvious correlation with the world as we typically know it, but a newly defined one. When we move away from copying reality, AR in general will be liberated from the burden of simulating the real, with the creative doors wide open to new modes of expression <a contenteditable="false" data-primary="smell and taste, digital" data-secondary="future applications for" data-startref="satd5faf" data-type="indexterm">&nbsp;</a>and <a contenteditable="false" data-primary="smell and taste, digital" data-startref="satd5" data-type="indexterm">&nbsp;</a>invention.</p>
</section>
</section>
