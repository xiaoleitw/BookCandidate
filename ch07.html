<section data-type="chapter" id="avatars_and_objects_as_living_change_age" xmlns="http://www.w3.org/1999/xhtml">
<h1>Avatars and Objects as Living Change Agents</h1>

<p>This second wave of Augmented Reality (AR) is driven by a contextual understanding and interaction with your surroundings. Your environment becomes a mirror to your being, adapting to your needs. Your surroundings become attuned and responsive, delivering a personalized, relevant, and meaningful experience. AR is no longer just a layering atop reality, it now becomes a transformation of reality. This chapter looks at how avatars, intelligent agents, objects, and materials are becoming living contextual change agents: learning, growing, predicting, and shape-shifting in order to add value to daily life and extend our humanity in new ways.</p>

<section data-type="sect1" id="the_ultimate_selfie">
<h1>The Ultimate Selfie</h1>

<p>With <a contenteditable="false" data-primary="avatars" data-secondary="humanizing" data-type="indexterm" id="av7h">&nbsp;</a><a contenteditable="false" data-primary="The Ultimate Selfie" data-primary-sortas="ultimate" data-type="indexterm" id="tus7">&nbsp;</a><a contenteditable="false" data-primary="avatars" data-secondary="The Ultimate Selfie" data-secondary-sortas="ultimate" data-type="indexterm" id="av7tus">&nbsp;</a><a contenteditable="false" data-primary="avatars" data-type="indexterm" id="av7">&nbsp;</a>the advancements today in AR and <a contenteditable="false" data-primary="Artificial Intelligence (AI)" data-type="indexterm">&nbsp;</a>Artificial Intelligence (AI), we could be closer than ever to <a contenteditable="false" data-primary="Virtual Reality (VR)" data-secondary="avatars" data-see="avatars" data-type="indexterm">&nbsp;</a>virtual human avatars that not only resemble us physically, but learn our behaviors and act on our behalf. “The ultimate selfie” is a concept from 2014 by artist, scientist, and Virtual Reality (VR) pioneer Dr. <a contenteditable="false" data-primary="Morie, Jacquelyn Ford" data-type="indexterm" id="mjf7">&nbsp;</a>Jacquelyn Ford Morie.<span data-type="footnote" id="fn20">Jacquelyn Ford Morie, “The Ultimate Selfie: Musings on the future of our human identity.” <em>23rd Annual Conference On Behavior Representation in Modeling and Simulation, Brims</em> (2014): 93-102.</span> The Ultimate Selfie is a modern AI agent that learns how we behave as we use it, and it could even become a legacy we leave behind after our human life span. “If and when we get to the stage where we have avatars that learn from us while we use them, and can stand in as surrogates for us when we are not there, then why couldn’t they continue to exist after we are gone?” asks Dr. Morie. “Imagine our descendants talking to their ancestors, asking for advice, or about family history, and more. This then, is the Ultimate Selfie.”<span data-type="footnote" id="fn21">Ibid, 100.</span></p>

<p>Dr. Morie identifies five important trends that fuel the concept for The Ultimate Selfie. The first trend is the increased precision in capturing the working data of our bodies. She points to the <a href="http://bit.ly/2w9X7Sx">Quantified Self</a> movement, <a contenteditable="false" data-primary="Quantified Self movement" data-type="indexterm">&nbsp;</a>which has become a mainstream phenomenon incorporating wearable technology and sensors into data acquisition of your daily life. “This trend encompasses everything from clip-ons and wristbands to devices that become part of our everyday attire, woven into clothing. Eventually we might see them implanted in our bodies in a true trans-humanist fashion,” says Dr. Morie.<span data-type="footnote" id="fn22">Ibid, 94.</span></p>

<p>The second trend is an increased capturing of our outward form. “We now have sophisticated ways to digitize not only our three-dimensional (3-D) shape in minute detail, but also intricate components of our appearance,” says Dr. Morie. “At least a dozen companies are focused on avatar creation now, and it is not difficult to find a place to have your entire body scanned.” She believes that we might see the time soon when every person will have a 3-D scan made throughout life, replacing snapshot pictures as the primary type of keepsake. The demand for more realistic avatar representations is only going to grow, and having a faithful 3-D avatar will be crucial.</p>

<p>The capability for your avatar to actually use your <a href="https://youtu.be/MMa2oT1wMIs">facial expressions</a> and body movements as part of its presentation in a virtual environment adds realism to The Ultimate Selfie. Companies like <a href="http://highfidelity.io/">High Fidelity</a>, a VR platform for users to create and deploy virtual worlds (founded in April 2013 by Second Life founder and former CEO Philip Rosedale), have made great strides in this area. <a href="http://www.quantumcapture.com/">Quantum Capture</a> and <a href="https://www.soulmachines.com/">Soul Machines</a> are other companies working in this space to help humanize avatars. This points to the third trend in which new sensing devices allow us to capture our unique behavioral motions. This includes motion capture techniques ranging from consumer depth sensing cameras like <a contenteditable="false" data-primary="Kinect" data-type="indexterm">&nbsp;</a>Kinect used with at-home computer games, to more sophisticated systems that use full body suits in Hollywood <a contenteditable="false" data-primary="film" data-type="indexterm">&nbsp;</a>film productions where human actors play computer-generated characters, like the Na’vi in the <a contenteditable="false" data-primary="film" data-secondary="Avatar" data-type="indexterm">&nbsp;</a>film <em>Avatar.</em></p>

<p>The fourth trend focuses on how to display the complex data gathered in the first three trends. Dr. Morie’s concept for The Ultimate Selfie predates relevant contemporary AR displays like the<a contenteditable="false" data-primary="HoloLens" data-type="indexterm">&nbsp;</a> HoloLens, which could be used to interact with The Ultimate Selfie. Although the HoloLens does not yet integrate data from our bodies (the first trend Dr. Morie identifies), it could one day. Microsoft has filed a patent<span data-type="footnote">McCulloch, et al., <a href="http://bit.ly/2u1Cp5U">Augmented reality help</a>, US Patent 9,030,495, filed November 21, 2012, and issued May 12, 2015.</span> for adding a biometric data sensor system to HoloLens to monitor and respond to stress levels, using the wearer’s heart rate, perspiration production, brainwave activity, and other body signals. This could help in training The Ultimate Selfie how your body and mind respond to specific situations while you’re wearing HoloLens.</p>

<p>The fifth trend Dr. Morie identifies is “our <a contenteditable="false" data-primary="teleconferencing persona" data-type="indexterm">&nbsp;</a>teleconferencing persona.” She describes how we’ve become accustomed to seeing others with technologies like video conferencing in boardrooms, <a href="https://suitabletech.com/">BEAM</a> telepresence robots, Skype, and FaceTime on our computers and smartphones, allowing us to interact over geographically separated locales. Our teleconferencing persona are being applied in AR today with systems like Holoportation and HoloLens Skype (as discussed in the previous chapter) and could merge with The Ultimate Selfie.</p>

<p>Dr. Morie believes these five trends enable us to capture and project so much of our human form and can help us to personalize a range of human-centered needs, including being in two places at once. This goal has practical uses and Dr. Morie gives an example of astronauts using The Ultimate Selfie. “When future astronauts travel on long duration space missions, which are expected to be a reality in the coming decade, they will not have the capability to video conference with their earth-based friends and family in real time, as they do now when they are deployed to the International Space Station. NASA is investigating using virtual worlds to help with the social and psychological isolation such astronauts might encounter when separated from earth and real time human contact on what could be a three-year mission with up to 40-minute communication delays.”<span data-type="footnote" id="fn23">Morie, “The Ultimate Selfie,” 98.</span></p>

<p>The Ultimate Selfie could also be beneficial for those who are shut in, are not mobile, or are in isolated communities. Dr. Morie says:</p>

<blockquote>
<p>Being able to go into a virtual environment with one’s avatar, and meet up with relatives or friends in other parts of the globe, can lead to a higher quality and better maintenance of human-to-human relationships. The next best thing to being there may well be your digital Selfie as this is a fully embodied representation of yourself.</p>
</blockquote>

<p>She points out how this is possible right now with a general or <a contenteditable="false" data-primary="customizable experiences" data-type="indexterm">&nbsp;</a>customized avatar you inhabit in real time in a social VR platform. “In the future, such 3-D digital selfies will actually learn to behave like you and will be capable of more complex interactions with others,” she adds.</p>

<p>It will take some time before we arrive at the stage when an avatar can become a true surrogate. To build up to that, an avatar will need to learn from you when you are using it so it can continue that behavior when you are not using it. In essence, you would be programming your digital AI replica by training it. One approach is to record your common actions so that an avatar can be scripted to play these back when you are not logged in to your representation. Our avatars can also learn from us with social media, which we use to record our life events. Your Facebook timeline, for instance, encapsulates what you have done and what is important to you. Dr. Morie asks, “What if eventually, the timeline was replaced by that learning avatar—The Ultimate Selfie? I think this could happen with the right focus on the research to make it so.”<span data-type="footnote" id="fn24">Ibid, 101.</span></p>

<p>For The Ultimate Selfie to become a reality as Dr. Morie envisions it, she says we need a much more complex AI architecture underpinning these avatars so they really do learn and retain information and actions that happen during their use. She believes this is the greatest challenge. “We don’t want a general AI agent, but one that grows and evolves specific to the person who is using that<a contenteditable="false" data-primary="Morie, Jacquelyn Ford" data-startref="mjf7" data-type="indexterm">&nbsp;</a> <a contenteditable="false" data-primary="avatars" data-secondary="humanizing" data-startref="av7h" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="The Ultimate Selfie" data-primary-sortas="ultimate" data-startref="tus7" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="avatars" data-secondary="The Ultimate Selfie" data-secondary-sortas="ultimate" data-startref="av7tus" data-type="indexterm">&nbsp;</a>avatar,” says Dr. Morie.</p>
</section>

<section data-type="sect1" id="aiapostrophell_be_right_back">
<h1>AI’ll Be Right Back</h1>

<p><a href="http://eterni.me/">Eterni.me</a> is an <a contenteditable="false" data-primary="Eterni.me" data-type="indexterm" id="eterni7">&nbsp;</a>MIT <a contenteditable="false" data-primary="avatars" data-secondary="immortality and" data-type="indexterm" id="av7ia">&nbsp;</a>startup that wants to help you become virtually immortal. The Eterni.me website reads, “It generates a virtual YOU, an avatar that emulates your personality and can interact with, and offers information and advice to your family and friends after you pass away. It’s like a Skype chat from the past.”</p>

<p>Eterni.me bares an eerie resemblance to the BBC’s Channel 4 television series <a contenteditable="false" data-primary="Black Mirror" data-type="indexterm">&nbsp;</a><em>Black Mirror</em>, specifically series 2, episode 1, “Be Right Back,” in which widowed Martha engages with the latest technology to communicate with her recently deceased husband, Ash. However, it’s not actually Ash; it’s a simulation powered by an AI program that gathers information about Ash through social media profiles and past online communications such as emails. Martha begins by chatting with virtual Ash and is able to later speak with him on the phone after uploading video files of him, from which the AI learns his voice. Eterni.me hopes to immortalize you in a similar fashion by collecting “almost everything you create during your lifetime and processes this huge amount of information using complex AI Algorithms.”</p>

<p>In an article on Eterni.me in Fast Company, Adele Peters writes,<span data-type="footnote">Adele Peters, <a href="http://bit.ly/2vudyfy">“A Creepy New Startup Wants To Create Living Avatars For Dead People,”</a> <em>Fast Company</em>, February 18, 2014.</span> “While the service promises to keep everything you do online so it’s never forgotten, it’s not clear that most people would want all of that information to live forever.” Commenting on how our current generation now documents “every meal on Instagram and every thought on Twitter,” Peters asks, “What do we want to happen to that information when we’re gone?”</p>

<p>Perhaps a job of the future will be an eternal avatar curator. In the 2004 film, <em>Final Cut</em>, directed by <a contenteditable="false" data-primary="film" data-secondary="Final Cut" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Final Cut (film)" data-type="indexterm">&nbsp;</a>Omar Naim and starring Robin Williams, Williams plays a “cutter,” someone who has the final edit over people’s recorded histories. A chip embedded in your body records all of your experiences over the course of your life; Williams’s job is to pore through all of the stored memories and produce a one-minute video of highlights.</p>

<p>Will Eterni.me’s AI algorithm be intelligent enough to make the final edit, distinguishing between your mundane and momentous experiences to accurately present your legacy? In <em>Black Mirror</em>, Martha ultimately tells simulated Ash, “You’re just a few ripples of you. There’s no history to you. You’re just a performance of stuff that he performed without thinking and it’s not enough.”</p>

<p>Marius <a contenteditable="false" data-primary="Ursache, Marius" data-type="indexterm">&nbsp;</a>Ursache, Eterni.me’s founder believes that collecting information is not enough. You will need to interact with your avatar, to help it make sense of the information, fine-tuning it. “People will train their avatars while they’re still alive,” says Ursache. “This is because we don’t have the algorithms and AI to allow re-creating a persona/its consciousness from only a few scattered emails or Facebook posts. To have the avatar be believable/true to life it will still take years of collecting data/training.”</p>

<p>Ursache describes the Eterni.me avatar as your personal biographer:</p>

<blockquote>
<p>It will want to learn as many things about you as possible, picking up cues from your social media, email, or smartphone. It will try to find meaning and context in everything you do, and it will try to have short chats with you every day in order to get more information about you. If you want to upload your thoughts, your personality and (maybe in the future) your consciousness, there’s no cable now. You will have to do it a little bit every day, for the rest of your life. Ten minutes every day will add up to thousands of hours telling your story. Fact by fact.</p>
</blockquote>

<p>Ursache equates your Eterni.me to a <a href="https://en.wikipedia.org/wiki/Tamagotchi">Tamagotchi</a> at the start. He explains,<span data-type="footnote">Marius Ursache, <a href="http://bit.ly/2unIKrx">“The Journey to Digital Immortality,”</a> October 23, 2015.</span> “It will only have small bursts of intelligence in the beginning, but the more you talk to it, and the more information you give it access to, the smarter it will become. Think of it as a kid who has to learn a lot until he/she turns into a beautiful human being.” This idea of an AI with an evolutionary trajectory, learning about you, growing, and becoming smarter as you use it, recalls director <a contenteditable="false" data-primary="film" data-secondary="Her" data-type="indexterm">&nbsp;</a>Spike Jonze’s film <a contenteditable="false" data-primary="Her (film)" data-type="indexterm">&nbsp;</a><em>Her</em> (2013), in which we are introduced to Samantha, the world’s first intelligent operating system. <em>Her</em> offers us a glimpse of our soon to be augmented life in which our devices come to learn and grow <a contenteditable="false" data-primary="Eterni.me" data-startref="eterni7" data-type="indexterm">&nbsp;</a>with us.</p>

<p>In addition to avatars, <a contenteditable="false" data-primary="intelligent agents" data-type="indexterm" id="ia7">&nbsp;</a>intelligent agents like Samantha, will come to act on our behalf. These intelligent agents will know us very well, learning our behaviors, our likes, our dislikes, our family and friends, even aware of our vital statistics. Futurist Brian David Johnson describes how for decades our relationship with technology has been based on an input–output model which has been command and control: if commands aren’t communicated correctly, or dare we have an accent, it breaks. Johnson believes we are entering more intelligent relationships with technology today. The computer knows you and how you are doing on any particular day and can deliver a personalized experience to increase your productivity.</p>

<p>He says this can “help us to be more human,” commenting on how in <em>Her</em>, Samantha nurses Theodore back to having more human relationships. Johnson states that technology is just a tool: we design our tools and imbue them with our sense of humanity and our values. We can have the ability to design our machines to take care of the people we love, allowing us to extend our humanity. He calls this designing “our better angels.” Johnson says the question we need to ask is, “What are we optimizing for?” The answer, he says, needs to be to make people’s lives better, and I wholeheartedly agree.</p>

<p>Dr. Genevieve Bell, <a contenteditable="false" data-primary="Bell, Genevieve" data-type="indexterm">&nbsp;</a>director of interaction and experience research at Intel, describes a world of computing in which we enter a more reciprocal relationship with technology where it begins to look after us, anticipating our needs, and doing things on our behalf. Dr. Bell’s predictions are echoed by Carolina Milanesi, Gartner’s research vice president. “If there is heavy traffic, it will wake you up early for a meeting with your boss, or simply send an apology if it is a meeting with your colleague. The smartphone will gather contextual information from its calendar, its sensors, the user’s location and personal data,” says Milanesi.<span data-type="footnote"><a href="http://www.gartner.com/newsroom/id/2621915">“Gartner Says by 2017 Your Smartphone Will Be Smarter Than You,”</a> November 12, 2013.</span></p>

<p>Gartner’s research claims this will work with initial services being performed “automatically” to assist generally with menial tasks that are significantly time consuming such as time-bound events, like calendaring, or responding to mundane email messages. A gradual confidence will be built in the outsourcing of menial tasks to the smartphone with an expectation that consumers will become more accustomed to smartphone apps and services taking control of other aspects of their lives. Milanesi says,<span data-type="footnote">Ibid.</span> “Phones will become our secret digital agent, but only if we are willing to provide the information they require.” Dr. Bell believes that we will go beyond “an interaction” with technology to entering a trusting “relationship” with our devices. Ten years from now, Bell says our devices will know us in a very different way by being intuitive about who we are.</p>

<p>Gartner calls this the era of <a contenteditable="false" data-primary="cognizant computing" data-type="indexterm">&nbsp;</a>cognizant computing and identifies the four stages as <em>Sync Me</em>, <em>See Me</em>, <em>Know Me</em>, <em>Be Me</em>. Sync Me and See Me are currently occurring, with Know Me and Be Me just ahead, as we see Samantha perform in <em>Her</em>. Sync Me stores copies of your digital assets, which are kept in synchronization across all contexts and end points. See Me knows where you are currently and where you have been in both the real world and on the internet as well as understanding your mood and context to best provide services. Know Me understands what you need and want, proactively, and presents it to you, with Be Me as the final step in which the smart device acts on your behalf based on learning. With access to all of Theodore’s emails, files, and other personal information, Samantha’s tasks evolve from initially managing Theodore’s calendar to gathering some of the love letters he ghostwrites to send them to a publisher, acting on his behalf. Be Me is also the point at which an intelligent agent could transform into your eternal avatar or The Ultimate Selfie after you have passed, learning about you throughout your life and continuing on where you last left <a contenteditable="false" data-primary="intelligent agents" data-startref="ia7" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="avatars" data-secondary="immortality and" data-startref="av7ia" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="avatars" data-startref="av7" data-type="indexterm">&nbsp;</a>off.</p>
</section>

<section data-type="sect1" id="invoked_computing">
<h1>Invoked Computing</h1>

<p>From intelligent personas and avatars, we move to intelligent spaces and objects. <em>Invoked Computing</em> <a contenteditable="false" data-primary="Invoked Computing" data-type="indexterm" id="ic7">&nbsp;</a>is a concept from 2011 by researchers at Ishikawa Oku Laboratory at the University of Tokyo for an AR system that uses spatial audio and video to transform everyday objects into communication devices. By making a gesture to mimic a specific scenario for the device that you want to use, ordinary objects become activated to suit your needs. A <a href="https://youtu.be/ZA6m2fxpxZk">proof of concept</a> has been created for a banana, which serves as a telephone, and a pizza box, which functions as a laptop computer.</p>

<p>To transform the banana into a phone, you would hold a banana and bring it close to your ear. An AR system recognizes your gesture and the object, with hidden directional microphones and speakers enabling the banana to function as a real handset. To invoke a laptop, you would open a pizza box and begin typing on the cardboard. Projected video and audio would alter the box to function as a laptop. The research group plans to broaden the range of recognized gestures and objects in the future, with the ultimate goal of creating a ubiquitous AR system that understands your wants and needs.</p>

<p>But you might be asking: why would I want to use a banana as a phone, or a pizza box as a laptop? Invoked Computing presents a scenario in which communication technology becomes ubiquitous and is no longer reliant on specific objects. Think about the last time you unintentionally left your smartphone at home. How did you feel? Depending on your attachment and reliance on your smartphone for daily living, you might have felt disconnected, unable to complete tasks, and perhaps even naked. Invoked Computing introduces an opportunity for a new convenience wherein you no longer need to carry communication devices with you; their uses become transferred to any object at hand in the environment as you need them.</p>

<p>With Invoked Computing, new functions are now layered atop ordinary objects that otherwise do not possess those traits. The banana is imagined and activated as a working telephone in one moment, returning to its inanimate state afterward. This is the beginning of a new era of responsive objects and environments that are on demand, context-dependent, and needs driven. Invoked Computing could be combined with AI to create a predictive environment based on your contextual needs.</p>

<p>Invoked Computing presents a world that is still rooted in physical objects, yet these objects are now dynamic with changing states. The emphasis is placed on the action and output of the object, as opposed to the physical qualities of the object itself, which is typically designed for a specific, often single purpose. Invoked Computing might come to reframe and change industrial design where any object can now perform any desired task. Rather than a collection of various electronics, appliances, or tools, imagine only using a few objects that could transform into anything you require. Futurist Bruce Sterling comments on how Invoked Computing affords the possibilities for sustainability and no material footprint because you can invoke and access everything you <a contenteditable="false" data-primary="Invoked Computing" data-startref="ic7" data-type="indexterm">&nbsp;</a>need.</p>
</section>

<section data-type="sect1" id="four-d_printing">
<h1>4-D Printing</h1>

<p>Whereas <a contenteditable="false" data-primary="4-D printing" data-primary-sortas="four d" data-type="indexterm" id="fourdp7">&nbsp;</a>Invoked Computing uses an audio and video projection system, <em>4-D Printing</em> is a concept by computer scientist <a contenteditable="false" data-primary="Tibbits, Skylar" data-type="indexterm" id="ts7">&nbsp;</a>Skylar Tibbits in which augmentation is built in to physical materials, allowing objects to grow and adapt. Like the banana phone example, 4-D Printing is not AR as we traditionally think of it, but points to an expanded idea of contextually responsive and evolving objects that enable a newfound interaction, immersion, and integration with our environment.</p>

<p>Tibbits is the director of the MIT Self-Assembly Lab where his team is working on 4-D Printing to enable “smart objects” that can self-assemble or transform when there is a change in the environment. Tibbits states,<span data-type="footnote"><a href="http://www.bbc.com/future/story/20130709-buildings-that-can-make-themselve">“4D printing: buildings that can change over time,”</a> <em>BBC</em>, July 11, 2013.</span> “The emerging technology of 4-D printing—where 3-D–printed material changes shape over time—means we may be able to build things that can adapt to our use or the environment around them.” Tibbits describes the fourth dimension as the idea of responding to time and printing something that is not static, but can evolve, and has a built-in resilience. “This is a whole new paradigm for how we make things. And a paradigm for after we make things, how they can be more resilient and adapt on their own,” he says.</p>

<p>The implications for 4-D printing could even be life-saving, using the technology in emergency zones to assist with disaster relief. 4-D printing could be used to construct pipes that shrink or expand based on their contact with water; to accommodate the runoff from a hurricane, they might grow in size and then contract when the emergency is over. 4-D printing could also be applied to disaster housing or refugee camps where structures can assemble themselves in places where unskilled laborers might be the only help available.</p>

<p>To make some of these concepts into a reality, Tibbits’s lab is working with 3-D printing manufacturer Stratasys. <a contenteditable="false" data-primary="Stratasys" data-type="indexterm">&nbsp;</a>Stratasys has developed a printing material that, when placed in water, expands by 150 percent. Tibbits and his team are applying geometry to enable precision in how an object can unfold and form specific angles rather than just swelling up in size. This differs from how a 3-D printer traditionally works with a blueprint; to make something 4-D, the printer is fed a geometric code with measurements that dictate how the printed matter should change shape when confronted with external forces like water, movement, or a change in temperature. The geometric code defines the direction, angles, and number of times the material can curl and bend.</p>

<p>Tibbits <a href="http://bit.ly/2vlRWRK">demonstrated</a> the concept of 4-D printing at a TED talk in 2013 with a single strand of printed material folding, on its own, into the word “MIT.” He points out how scientists have been able to program physical and biological materials in nanotechnology to change their shapes and properties. Tibbits acknowledges that making this happen on a human scale is far more challenging, but that hasn’t stopped his lab from exploring the possibilities. He envisions construction as a potential area to apply self-assembling materials. Tibbits says his lab is working closely with industry partners to incorporate the concept into their businesses.</p>

<p>Tibbits also sees a consumer future with 4-D printing in sportswear. He gives the example of a pair of sneakers that could change their function and shape in response to the context in which they are used:</p>

<blockquote>
<p>If I start running, the sneakers should adapt to being running shoes. If I play basketball, they adapt to support my ankles more. If I go on grass, they should grow cleats or become waterproof if it’s raining. It’s not like the shoe would understand that you’re playing basketball, of course, but it can tell what kind of energy or what type of forces are being applied by your foot. It could transform based on pressure. Or it could be moisture or temperature change.</p>
</blockquote>

<p>This concept can also be applied on a larger scale to architecture whereby a building could adjust in form, structure, and use, in conversation with the changing environment, including the people occupying and interacting with the building. For instance, the building could adapt according to weather, time of day, capacity of people, and social scenarios. Such a 4-D printing system could change the way architects and engineers design and manufacture buildings.</p>

<p>Tibbits asks,<span data-type="footnote">Ibid.</span> “What if the world was about people, machines, and materials that collaborate? Each one has new things to offer and we could have a much more rich conversation between those.” As AR evolves, the technology will strive to be a real time two-way conversation with the user’s changing needs and surroundings. AR will no longer be a gimmicky overlay, but a dynamic, meaningful, and responsive on-demand experience with the user in an ongoing conversation with his or her environment. Tibbits’s concept could be integrated into an AR experience with the ability for objects to have built-in adaptive behaviors, enabling them to grow and adjust as living responsive objects to situational <a contenteditable="false" data-primary="Tibbits, Skylar" data-startref="ts7" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="4-D printing" data-primary-sortas="four d" data-startref="fourdp7" data-type="indexterm">&nbsp;</a>changes.</p>
</section>

<section data-type="sect1" id="the_reality_editor">
<h1>The Reality Editor</h1>

<p>Valentin Heun, a <a contenteditable="false" data-primary="Heun, Valentin" data-type="indexterm" id="hv7">&nbsp;</a>researcher at <a contenteditable="false" data-primary="Reality Editor" data-type="indexterm" id="re7">&nbsp;</a>the MIT Media Lab’s Fluid Interfaces Group, echoed this sentiment of AR as a two-way conversation between the user and environment. In his 2016 talk at Augmented World Expo<a contenteditable="false" data-type="indexterm" data-primary="Augmented World Expo (AWE)">&nbsp;</a> in Silicon Valley, California, Heun stated:</p>

<blockquote>
<p>The interesting part is when you just look at AR as a one-way direction, you have a medium for consumption of data, of video, and so on. But when you create a bidirectional link, you suddenly end up with a very powerful tool, a tool that’s basically a digital version of a swiss army knife that allows you to change the functionalities of the world.</p>
</blockquote>

<p>Heun has developed an iOS app called the <a href="http://www.realityeditor.org/">Reality Editor</a> with the ambition to reprogram the physical world using AR. The app allows you to link smart physical objects around you by drawing connections between them with your finger on the screen of your smartphone or tablet. “It’s really just a start; it’s a small first step to figure out how can we can make physical things around us connected, and nonstatic, and how do we interact with them? Because right now, we cannot,” says Heun.<span data-type="footnote">Will Shandling, <a href="http://bit.ly/2wpidvB">“Indistinguishable Reality: A Conversation with Reality Editor’s Valentin Heun,”</a> <em>Designation Blog</em>, February 19, 2016.</span> He refers to the Reality Editor as a digital tool, a screwdriver that allows you to connect and manipulate how physical things behave.</p>

<p>The app doesn’t work with out-of-the-box consumer products yet; it runs on an open source platform called <a contenteditable="false" data-primary="Open Hybrid network" data-type="indexterm">&nbsp;</a>Open Hybrid with which you can map a virtual interface directly onto a physical object using AR. Presently, this is done by creating a sticker (similar to a QR code) to apply on the physical objects that you want to connect, but Heun says this won’t be necessary in the future, with image recognition built in to the app. Heun used the Open Hybrid network to create a working demonstration in which he connects a lamp, a chair, and his car to streamline the process of leaving work. He says, “Imagine that chair that you’re sitting on actually is responsive to the environment, so when you leave, the environment reacts on you leaving.” Standing up from his chair at work and walking out the door, the lamp turns off, his car is activated and starts, with the air conditioning in the car at just the right temperature.</p>

<p>Using Open Hybrid, you can also take the functionality of an object and add it to another object by drawing a line from one component to the next in the app. For example, if you want your food processor to have a timer, point the camera on your smartphone or tablet at the appliance and use the Reality Editor app to draw a line from another object that has a timer, such as a toaster, to the food processor. The two objects then would be automatically connected over the Open Hybrid server. Like Invoked Computing and 4-D Printing, the Reality Editor could change our concept of affordances and objects that have an action that is possible beyond its physical form.</p>

<p>Heun speaks to the difference between physical and virtual objects: physical objects typically have a static behavior, whereas virtual objects are nonstatic, can be changed all the time, and can have different properties. “So, what is interesting is, if you have a physical object that is NOT fully static—one that can actually change how it operates, how it is used after it comes ‘out of the factory’,” observes Heun. “That is the challenge now: to see, from a design perspective, where we’re going with this technology, what can we do with it.”</p>

<p>AR has the power to transform the way we experience the world and even design itself. I believe we have two important questions to ask as we begin to explore these new virtual affordances that stretch beyond traditional physical forms: Now that we can design anything, what will we create? How can we use these newfound capabilities to best enrich, evolve, and elevate <a contenteditable="false" data-primary="Reality Editor" data-startref="re7" data-type="indexterm">&nbsp;</a><a contenteditable="false" data-primary="Heun, Valentin" data-startref="hv7" data-type="indexterm">&nbsp;</a>humankind?</p>
</section>
</section>
